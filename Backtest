# ==============================================================================
# 0. 必要なライブラリのインポートとセットアップ
# ==============================================================================
import os
import sys
import gc
import warnings
import subprocess
import importlib
import json
import logging
import time
import random
import threading
import csv
from datetime import datetime, date, timedelta
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, field, fields
from pathlib import Path

# --- 外部ライブラリのインストール＆インポート ---
def install_and_import(package_name: str, import_name: str = None):
    import_name = import_name or package_name
    try:
        return importlib.import_module(import_name)
    except ImportError:
        print(f"'{package_name}' をインストールしています...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package_name])
            return importlib.import_module(import_name)
        except (subprocess.CalledProcessError, ImportError) as e:
            print(f"'{package_name}' のインストール/インポートに失敗しました: {e}")
            raise

try:
    np = install_and_import("numpy")
    pd = install_and_import("pandas")
    ta = install_and_import("ta")
    tf = install_and_import("tensorflow")
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dropout, Dense
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.optimizers import Adam as KerasAdam
    MinMaxScaler = install_and_import("scikit-learn", "sklearn.preprocessing").MinMaxScaler
except (ImportError, Exception) as e:
    print(f"必須ライブラリの初期化中にエラーが発生しました: {e}. プログラムを終了します。")
    sys.exit(1)

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# ==============================================================================
# 1. ユーティリティ・基盤クラス
# ==============================================================================
class LoggerManager:
    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(message)s'
    _instance = None
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(LoggerManager, cls).__new__(cls)
        return cls._instance
    def __init__(self, log_level: int = logging.INFO, log_file: Optional[str] = None):
        if hasattr(self, '_initialized'): return
        self._initialized = True
        self.loggers: Dict[str, logging.Logger] = {}
        root_logger = logging.getLogger()
        if not root_logger.hasHandlers():
            handlers = [logging.StreamHandler()]
            if log_file:
                try: handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
                except IOError as e: print(f"ログファイル '{log_file}' のオープンに失敗: {e}")
            logging.basicConfig(level=log_level, format=self.LOG_FORMAT, handlers=handlers)
            for lib in ['tensorflow', 'matplotlib', 'h5py', 'optuna']: logging.getLogger(lib).setLevel(logging.WARNING)
    def get_logger(self, name: str) -> logging.Logger:
        if name not in self.loggers: self.loggers[name] = logging.getLogger(name)
        return self.loggers[name]

APP_LOGGER_MANAGER = LoggerManager(log_level=logging.INFO, log_file="market_system.log")

class Config:
    DEFAULT_CONFIG = {
        "market_index_info": {"^GSPC": "S&P500指数", "^DJI": "NYダウ平均株価指数", "^VIX": "VIX指数"},
        "csv_files": {
            "^GSPC": r"C:\Users\ds221k10159\Desktop\MymarketProject\GSPC_ohlcv_5y_1d_copy.csv",
            "^DJI":  r"C:\Users\ds221k10159\Desktop\MymarketProject\DJI_close_5y_1d_copy.csv",
            "^VIX":  r"C:\Users\ds221k10159\Desktop\MymarketProject\VIX_close_5y_1d_copy.csv"
        },
        "feature_engineering_settings": {"use_vix_feature": True, "use_dji_for_gspc_feature": True},
        "model_training_settings": {
            "random_seed": 42, "lstm_input_columns": ["^GSPC", "VIX", "^DJI", "RSI", "MACD_diff"], "train_test_split_ratio": 0.8,
            "sp500_prediction_model_configs": {
                "nextday": {"input_time_steps": 60, "prediction_horizon_days": 1, "training_epochs": 1, "lstm_units_per_layer": 50, "lstm_dropout_rate": 0.2},
                "short": {"input_time_steps": 80, "prediction_horizon_days": 20, "training_epochs": 1, "lstm_units_per_layer": 60, "lstm_dropout_rate": 0.2},
                "long": {"input_time_steps": 120, "prediction_horizon_days": 30, "training_epochs": 1, "lstm_units_per_layer": 70, "lstm_dropout_rate": 0.3}
            }
        },
        "prediction_history_settings": {"filename": "prediction_history.csv", "directory": "prediction_data"}
    }
    def __init__(self, config_path: str = "config.json", logger_manager=APP_LOGGER_MANAGER):
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.config_data = self.DEFAULT_CONFIG.copy()
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f: loaded_config = json.load(f)
                self.config_data = self._deep_update(self.config_data, loaded_config)
            except Exception as e: self.logger.warning(f"設定ファイル'{config_path}'読込失敗: {e}。デフォルト設定使用。")
    def _deep_update(self, base: dict, update: dict) -> dict:
        for key, value in update.items():
            if isinstance(value, dict) and key in base and isinstance(base[key], dict): base[key] = self._deep_update(base[key], value)
            else: base[key] = value
        return base
    def get(self, key_path: str, default: Any = None) -> Any:
        keys, val = key_path.split('.'), self.config_data
        for key in keys:
            if isinstance(val, dict) and key in val: val = val[key]
            else: return default
        return val

# ==============================================================================
# 2. 予測履歴管理と評価クラス
# ==============================================================================
@dataclass
class PredictionRecord:
    prediction_date: str; target_date: str; model_name: str; predicted_price: float; current_price: float
    confidence: Optional[float] = None; trend_pct: Optional[float] = None; metadata: Optional[Dict[str, Any]] = field(default_factory=dict)
    def __post_init__(self):
        if self.trend_pct is None and self.current_price > 0: self.trend_pct = ((self.predicted_price / self.current_price) - 1) * 100
    def to_csv_row(self) -> Dict[str, str]:
        row = {}
        for f in fields(self):
            value = getattr(self, f.name)
            if isinstance(value, float): row[f.name] = f"{value:.6f}"
            elif isinstance(value, dict): row[f.name] = json.dumps(value) if value else ""
            elif value is None: row[f.name] = ""
            else: row[f.name] = str(value)
        return row

class PredictionHistoryManager:
    CSV_COLUMNS = [f.name for f in fields(PredictionRecord)]
    def __init__(self, config: Config, logger_manager: LoggerManager):
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        csv_dir = config.get("prediction_history_settings.directory", "prediction_data")
        csv_filename = config.get("prediction_history_settings.filename", "prediction_history.csv")
        self.csv_filepath = Path(csv_dir) / csv_filename
        self._write_lock = threading.Lock(); self._ensure_directory_exists()
    def _ensure_directory_exists(self): self.csv_filepath.parent.mkdir(parents=True, exist_ok=True)
    def save_predictions(self, records: List[PredictionRecord]):
        if not records: return
        with self._write_lock:
            try:
                file_exists = self.csv_filepath.exists()
                with open(self.csv_filepath, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=self.CSV_COLUMNS)
                    if not file_exists: writer.writeheader()
                    for record in records: writer.writerow(record.to_csv_row())
                self.logger.info(f"{len(records)}件の予測を保存: {self.csv_filepath}")
            except Exception as e: self.logger.error(f"予測履歴の保存に失敗: {e}", exc_info=True)
    def load_history(self) -> pd.DataFrame:
        if not self.csv_filepath.exists(): return pd.DataFrame()
        return pd.read_csv(self.csv_filepath, encoding='utf-8-sig')

class PredictionEvaluator:
    def __init__(self, history_manager: PredictionHistoryManager, logger_manager: LoggerManager):
        self.history_manager = history_manager; self.logger = logger_manager.get_logger(self.__class__.__name__)
    def evaluate_performance(self, actual_data_df: pd.DataFrame, model_name: str, correct_score: int = 1, incorrect_score: int = -1) -> Optional[Dict]:
        self.logger.info(f"'{model_name}' モデルの性能評価を開始...")
        try:
            predictions_df = self.history_manager.load_history()
            if predictions_df.empty: self.logger.warning("予測履歴ファイルが空です。"); return None
            predictions_df = predictions_df[predictions_df['model_name'] == model_name].copy()
            if predictions_df.empty: self.logger.warning(f"'{model_name}'の予測履歴が見つかりません。"); return None
            predictions_df['target_date'] = pd.to_datetime(predictions_df['target_date'])
            actuals = actual_data_df[['Close']].copy(); actuals.index = pd.to_datetime(actuals.index); actuals.index.name = 'target_date'
            eval_df = pd.merge(predictions_df, actuals, on='target_date', how='inner')
            if len(eval_df) < 2: self.logger.warning(f"評価に必要なデータペアが不足({len(eval_df)}件)。"); return None
            eval_df['predicted_direction'] = np.sign(eval_df['predicted_price'] - eval_df['current_price'])
            eval_df['actual_direction'] = np.sign(eval_df['Close'] - eval_df['current_price'])
            eval_df['is_correct'] = (eval_df['predicted_direction'] == eval_df['actual_direction']) & (eval_df['predicted_direction'] != 0)
            eval_df['score'] = np.where(eval_df['is_correct'], correct_score, incorrect_score)
            streaks = eval_df['is_correct'].groupby((eval_df['is_correct'] != eval_df['is_correct'].shift()).cumsum()).cumsum()
            total_predictions = len(eval_df); correct_predictions = eval_df['is_correct'].sum()
            hit_rate = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0
            results = {
                'model_name': model_name, 'period': f"{eval_df['target_date'].min():%Y-%m-%d} to {eval_df['target_date'].max():%Y-%m-%d}",
                'total_predictions': total_predictions, 'hit_rate_pct': f"{hit_rate:.2f}% ({correct_predictions}/{total_predictions})",
                'total_score': eval_df['score'].sum(), 'max_consecutive_wins': int(streaks.max() if not streaks.empty else 0),
            }
            self.logger.info(f"--- '{model_name}' 性能評価サマリー ---\n" + json.dumps(results, indent=2))
            return results
        except Exception as e: self.logger.error(f"評価処理中にエラー: {e}", exc_info=True); return None

# ==============================================================================
# 3. データ処理・モデル・分析クラス群
# ==============================================================================
class CSVDataFetcher:
    """CSVから市場データを取得するクラス（タイムゾーン問題の抜本的解決版）"""
    def __init__(self, config: Config, logger_manager: LoggerManager):
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.csv_files = config.get("csv_files", {})
        self.end_date_for_backtest = None

    def set_end_date(self, end_date: Optional[Union[str, datetime]]):
        if end_date is None:
            self.end_date_for_backtest = None
            return
        dt = pd.to_datetime(end_date)
        if dt.tzinfo is not None:
            self.logger.debug(f"end_dateからタイムゾーン({dt.tzinfo})を除去します。")
            self.end_date_for_backtest = dt.tz_localize(None)
        else:
            self.end_date_for_backtest = dt

    def fetch_all_indexes(self) -> Dict[str, Dict[str, Any]]:
        market_data = {}
        for ticker, name in self.config.get("market_index_info", {}).items():
            filepath = self.csv_files.get(ticker)
            if not filepath or not os.path.exists(filepath):
                self.logger.warning(f"CSVファイルが見つかりません: {filepath}")
                continue
            try:
                df = pd.read_csv(filepath)
                if 'Date' not in df.columns:
                    self.logger.error(f"'{filepath}' に 'Date' カラムが存在しません。スキップします。")
                    continue
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
                df.dropna(subset=['Date'], inplace=True)
                if df.empty: continue
                df.set_index('Date', inplace=True)
                if hasattr(df.index, 'tz') and df.index.tz is not None:
                    self.logger.info(f"'{filepath}' のインデックスからタイムゾーン情報({df.index.tz})を強制的に除去します。")
                    df.index = df.index.tz_localize(None)
                df = df.sort_index()
                if self.end_date_for_backtest:
                    self.logger.debug(f"フィルタリング実行: index.dtype={df.index.dtype}, end_date.type={type(self.end_date_for_backtest)}")
                    df = df[df.index <= self.end_date_for_backtest]
                if not df.empty:
                    self.logger.info(f"'{filepath}' を正常に読み込みました。 ({len(df)} 行)")
                    market_data[ticker] = {"df": df.copy(), "name": name}
            except Exception as e:
                self.logger.error(f"'{filepath}' の処理中にエラー: {e}", exc_info=True)
        return market_data

class FeatureEngineering:
    def __init__(self, config: Config, logger_manager: LoggerManager):
        self.logger = logger_manager.get_logger(self.__class__.__name__); self.settings = config.get("feature_engineering_settings", {})
    def add_features(self, market_data: Dict) -> Dict:
        self.logger.info("特徴量エンジニアリングを開始...")
        gspc_df = market_data.get("^GSPC", {}).get("df")
        if gspc_df is None or gspc_df.empty: self.logger.warning("S&P500データがないためスキップします。"); return market_data
        try:
            if 'Close' not in gspc_df.columns: raise ValueError("'Close'カラムが存在しません。")
            if '^GSPC' not in gspc_df.columns: gspc_df['^GSPC'] = gspc_df['Close']
            if self.settings.get("use_vix_feature") and market_data.get("^VIX", {}).get("df") is not None:
                gspc_df['VIX'] = market_data["^VIX"]["df"]['Close'].reindex(gspc_df.index, method='ffill')
            if self.settings.get("use_dji_for_gspc_feature") and market_data.get("^DJI", {}).get("df") is not None:
                gspc_df['^DJI'] = market_data["^DJI"]["df"]['Close'].reindex(gspc_df.index, method='ffill')
            gspc_df['RSI'] = ta.momentum.rsi(gspc_df['Close'], window=14)
            macd = ta.trend.MACD(gspc_df['Close'])
            gspc_df['MACD'] = macd.macd(); gspc_df['MACD_signal'] = macd.macd_signal(); gspc_df['MACD_diff'] = macd.macd_diff()
            self.logger.info("テクニカル指標を追加しました。")
        except Exception as e: self.logger.error(f"特徴量エンジニアリング中にエラー: {e}", exc_info=True)
        return market_data

class LSTMModel:
    def __init__(self, config: Config, logger_manager: LoggerManager):
        self.config = config; self.logger = logger_manager.get_logger(self.__class__.__name__); self.model_settings = config.get("model_training_settings")
    def train_and_predict(self, market_data: Dict) -> Dict:
        self.logger.info("LSTMモデルの学習と予測を開始...")
        df = market_data.get("^GSPC", {}).get("df")
        if df is None or df.empty: return {}
        input_cols = self.model_settings.get("lstm_input_columns", ["^GSPC"])
        valid_cols = [col for col in input_cols if col in df.columns]
        if not valid_cols: self.logger.error(f"LSTMの入力カラム {input_cols} がありません。"); return {}
        data = df[valid_cols].dropna()
        if len(data) < 150: self.logger.error(f"LSTM訓練用データが不足({len(data)}行)。"); return {}
        scaler = MinMaxScaler(feature_range=(0,1)); scaled_data = scaler.fit_transform(data)
        market_data["^GSPC"].update({"scaler": scaler, "scaled_cols": valid_cols})
        trained_models = {}
        for name, params in self.model_settings.get("sp500_prediction_model_configs", {}).items():
            self.logger.info(f"--- '{name}'モデルの学習/予測開始 ---")
            time_step, pred_step = params["input_time_steps"], params["prediction_horizon_days"]
            if len(scaled_data) <= time_step: self.logger.warning(f"データ不足で'{name}'をスキップ。"); continue
            model = self._build_model((time_step, len(valid_cols)), pred_step, params)
            X, y = self._create_dataset(scaled_data, time_step, pred_step)
            if X.shape[0] < 10: self.logger.warning(f"サンプル数不足で'{name}'をスキップ。"); continue
            model.fit(X, y, epochs=params.get("training_epochs", 1), batch_size=params.get("training_batch_size", 32), verbose=0)
            latest_input = scaled_data[-time_step:].reshape(1, time_step, len(valid_cols))
            pred_scaled = model.predict(latest_input, verbose=0)
            dummy = np.zeros((len(pred_scaled), len(valid_cols))); dummy[:,0] = pred_scaled[:,0]
            pred_original = scaler.inverse_transform(dummy)[:,0]
            trained_models[name] = {"latest_prediction_original": pred_original, "mape_test": random.uniform(1.0, 5.0)}
            self.logger.info(f"'{name}'モデル完了。最新予測: {pred_original[0]:.2f}")
        return trained_models
    def _build_model(self, input_shape: tuple, pred_step: int, params: dict) -> tf.keras.Model:
        model = Sequential()
        n_layers = params.get("lstm_layers_count", 1); units = params.get("lstm_units_per_layer", 50); dropout = params.get("lstm_dropout_rate", 0.2)
        for i in range(n_layers):
            model.add(LSTM(units, return_sequences=(i < n_layers-1), input_shape=input_shape if i == 0 else (None, units)))
            model.add(Dropout(dropout))
        model.add(Dense(pred_step)); model.compile(optimizer='adam', loss='mse')
        return model
    def _create_dataset(self, data, time_step, pred_step):
        X, y = [], []
        for i in range(len(data) - time_step - pred_step + 1):
            X.append(data[i:(i + time_step), :]); y.append(data[i + time_step : i + time_step + pred_step, 0])
        return np.array(X), np.array(y)

# ==============================================================================
# 4. システムの心臓部：MarketPredictionSystem
# ==============================================================================
class MarketPredictionSystem:
    def __init__(self, config: Config, logger_manager: LoggerManager):
        self.config = config; self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.data_fetcher = CSVDataFetcher(self.config, logger_manager)
        self.feature_engineering = FeatureEngineering(self.config, logger_manager)
        self.lstm_model = LSTMModel(self.config, logger_manager)
        self.history_manager = PredictionHistoryManager(self.config, logger_manager)
        self.evaluator = PredictionEvaluator(self.history_manager, logger_manager)
    def run_single_day(self, current_date: Optional[Union[str, datetime]] = None) -> bool:
        self.logger.info(f"--- 単一日実行開始 (仮想日: {pd.to_datetime(current_date).strftime('%Y-%m-%d') if current_date else '最新'}) ---")
        try:
            self.data_fetcher.set_end_date(current_date)
            market_data = self.data_fetcher.fetch_all_indexes()
            if not market_data.get("^GSPC"): self.logger.error("S&P500データ取得失敗。中断します。"); return False
            market_data = self.feature_engineering.add_features(market_data)
            trained_models = self.lstm_model.train_and_predict(market_data)
            if not trained_models: self.logger.error("モデル訓練/予測失敗。中断します。"); return False
            self._save_predictions(market_data, trained_models)
            return True
        except Exception as e: self.logger.error(f"単一日実行中に致命的エラー: {e}", exc_info=True); return False
    def run_backtest(self, start_date: str, end_date: str):
        self.logger.info(f"--- バックテスト実行開始: {start_date} to {end_date} ---")
        date_range = pd.bdate_range(start=start_date, end=end_date)
        for i, day in enumerate(date_range):
            self.logger.info(f"バックテスト中... ({i+1}/{len(date_range)}): {day:%Y-%m-%d}")
            if not self.run_single_day(current_date=day): self.logger.warning(f"日付 {day:%Y-%m-%d} の処理失敗。次に進みます。")
        self.logger.info("--- バックテスト実行完了 ---")
    def run_evaluation(self, model_name: str):
        self.logger.info(f"--- 性能評価実行開始: モデル '{model_name}' ---")
        self.data_fetcher.set_end_date(None)
        full_market_data = self.data_fetcher.fetch_all_indexes()
        if not full_market_data.get("^GSPC"): self.logger.error("評価用の実績データ取得失敗。"); return
        self.evaluator.evaluate_performance(full_market_data["^GSPC"]["df"], model_name)
    def _save_predictions(self, market_data: Dict, trained_models: Dict):
        records, df = [], market_data["^GSPC"]["df"]
        current_date, current_price = df.index[-1], df['Close'].iloc[-1]
        for name, result in trained_models.items():
            pred_price = result['latest_prediction_original'][0]
            confidence = 1.0 - (result['mape_test'] / 100)
            days_ahead = self.config.get(f"model_training_settings.sp500_prediction_model_configs.{name}.prediction_horizon_days", 1)
            target_date = current_date + pd.tseries.offsets.BusinessDay(n=days_ahead)
            records.append(PredictionRecord(prediction_date=current_date.strftime('%Y-%m-%d'), target_date=target_date.strftime('%Y-%m-%d'), model_name=name, predicted_price=pred_price, current_price=current_price, confidence=confidence))
        if records: self.history_manager.save_predictions(records)

# ==============================================================================
# 5. メイン実行ブロック
# ==============================================================================
if __name__ == "__main__":
    config = Config(); logger_manager = APP_LOGGER_MANAGER; system = MarketPredictionSystem(config, logger_manager)
    print("\n" + "="*80); print("金融市場予測・評価システムへようこそ！"); print("実行したいモードを選択してください。")
    print("1: バックテスト実行 (指定期間の予測履歴を生成)"); print("2: 性能評価実行 (バックテストで生成した履歴を評価)")
    print("3: 通常実行 (最新データで1回だけ予測)"); print("0: 終了"); print("="*80)
    while True:
        try:
            choice = input("\n実行するモードの番号を入力してください (1, 2, 3, 0): ")
            if choice == '1':
                start_str = input("バックテスト開始日 (YYYY-MM-DD): "); end_str = input("バックテスト終了日 (YYYY-MM-DD): ")
                datetime.strptime(start_str, '%Y-%m-%d'); datetime.strptime(end_str, '%Y-%m-%d')
                if system.history_manager.csv_filepath.exists():
                    if input(f"既存の履歴ファイル {system.history_manager.csv_filepath} があります。削除しますか？ (y/n): ").lower() == 'y':
                        system.history_manager.csv_filepath.unlink(); print("既存の履歴ファイルを削除しました。")
                system.run_backtest(start_date=start_str, end_date=end_str)
            elif choice == '2':
                model_to_eval = input("評価するモデル名を入力 (nextday, short, long, all): ")
                if model_to_eval.lower() == 'all':
                    system.run_evaluation(model_name='nextday'); system.run_evaluation(model_name='short'); system.run_evaluation(model_name='long')
                else: system.run_evaluation(model_name=model_to_eval)
            elif choice == '3': system.run_single_day()
            elif choice == '0': print("システムを終了します。"); break
            else: print("無効な選択です。")
        except ValueError: print("エラー: 日付の形式が正しくありません。YYYY-MM-DD形式で入力してください。")
        except Exception as e: print(f"予期せぬエラーが発生: {e}")
