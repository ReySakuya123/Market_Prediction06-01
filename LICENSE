import os
import gc
import warnings
import subprocess
import importlib
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple, List, Type, Union # Union を追加
import json
import logging
import time
import random # LSTMModelのset_random_seedで使用

# 必要な外部ライブラリのインストール＆インポート
def install_and_import(package_name: str, import_name: str = None, version_spec: Optional[str] = None):
    """
    パッケージをインストール（存在しない場合）してインポートする。
    バージョン指定も可能。
    """
    import_name = import_name or package_name
    try:
        module = importlib.import_module(import_name)
        # バージョンチェック (オプション)
        if version_spec and hasattr(module, '__version__'):
            from packaging.requirements import Requirement
            from packaging.version import parse as parse_version
            req = Requirement(f"{package_name}{version_spec}")
            if not req.specifier.contains(parse_version(module.__version__)):
                raise ImportError(f"{package_name}のバージョンが要求({version_spec})と異なります: {module.__version__}")
        return module
    except ImportError:
        package_to_install = package_name
        if version_spec:
            package_to_install += version_spec
        print(f"'{package_to_install}' をインストールしています...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package_to_install]) # sys.executable を使用
            return importlib.import_module(import_name)
        except subprocess.CalledProcessError as e:
            print(f"'{package_to_install}' のインストールに失敗しました: {e}")
            raise
        except Exception as e: # インポート後の予期せぬエラー
            print(f"'{import_name}' のインポート中にエラーが発生しました: {e}")
            raise

# --- 外部ライブラリのインポート ---
# ログ出力はLoggerManager初期化後に行うため、ここではprintを使用
try:
    import sys # install_and_importで使用
    np = install_and_import("numpy")
    pd = install_and_import("pandas")
    plt = install_and_import("matplotlib", "matplotlib.pyplot")
    from matplotlib.axes import Axes
    sns = install_and_import("seaborn")
    sklearn_preprocessing = install_and_import("scikit-learn", "sklearn.preprocessing") # パッケージ名修正
    MinMaxScaler = sklearn_preprocessing.MinMaxScaler
    stats = install_and_import("scipy").stats
    ta = install_and_import("ta")
    optuna = install_and_import("optuna")
    tf = install_and_import("tensorflow")
    from tensorflow.keras.models import Sequential, save_model, load_model
    from tensorflow.keras.layers import LSTM, Dropout, Dense
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.optimizers import Adam as KerasAdam # Adamを明示的にインポート
except ImportError as e:
    print(f"必須ライブラリのインポート/インストールに失敗しました: {e}. プログラムを終了します。")
    sys.exit(1) # 致命的なエラーとして終了
except Exception as e: # その他の予期せぬエラー
    print(f"ライブラリ初期化中に予期せぬエラー: {e}. プログラムを終了します。")
    sys.exit(1)


warnings.filterwarnings('ignore', category=FutureWarning) # TensorFlow等のFutureWarningを抑制
warnings.filterwarnings('ignore', category=UserWarning)   # Seaborn等のUserWarningを抑制

# --- CurlSession の条件付きエイリアス定義 ---
CurlSession: Optional[Type[Union[Any, Any]]] = None # requests.Session or curl_cffi.requests.Session
# Union[requests.Session, curl_cffi.requests.Session] のように具体的な型を書くのが理想だが、
# インポート失敗時のために Any も許容。None の可能性もあるため Optional
try:
    from curl_cffi.requests import Session as CurlCffiSession
    CurlSession = CurlCffiSession
    print("INFO: curl_cffi.requests.Session を CurlSession として使用します。")
except ImportError:
    try:
        from requests import Session as RequestsSession
        CurlSession = RequestsSession
        print("INFO: requests.Session を CurlSession として使用します (curl_cffi が見つかりませんでした)。")
    except ImportError:
        print("WARNING: curl_cffi と requests のどちらも見つかりませんでした。HTTPリクエスト機能が制限されます。")
        # CurlSession は None のまま


class LoggerManager:
    """ロギング管理クラス"""
    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(process)d - %(threadName)s - %(message)s'

    def __init__(self, log_level: int = logging.INFO, log_file: Optional[str] = None):
        self.loggers: Dict[str, logging.Logger] = {}
        self.log_level = log_level
        self.log_file = log_file
        self.performance_log: List[Dict[str, Any]] = []
        self._setup_root_logger()

    def _setup_root_logger(self):
        """ルートロガーの基本的な設定。basicConfigは一度だけ呼び出されるべき。"""
        # 既にハンドラが設定されているか確認
        root_logger = logging.getLogger()
        if not root_logger.hasHandlers():
            handlers = []
            stream_handler = logging.StreamHandler()
            stream_handler.setFormatter(logging.Formatter(self.LOG_FORMAT))
            handlers.append(stream_handler)

            if self.log_file:
                try:
                    file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
                    file_handler.setFormatter(logging.Formatter(self.LOG_FORMAT))
                    handlers.append(file_handler)
                except IOError as e:
                    print(f"ログファイル '{self.log_file}' のオープンに失敗: {e}. ファイルログは無効になります。")

            logging.basicConfig(level=self.log_level, handlers=handlers)
            # 外部ライブラリのログレベルを制御 (例: TensorFlow, matplotlib)
            logging.getLogger('tensorflow').setLevel(logging.WARNING)
            logging.getLogger('matplotlib').setLevel(logging.WARNING)
            logging.getLogger('h5py').setLevel(logging.WARNING)
            # print(f"ルートロガーを設定しました。レベル: {logging.getLevelName(self.log_level)}")
        else:
            # ルートロガーが既に設定済みの場合、レベルのみ調整 (必要であれば)
            root_logger.setLevel(self.log_level)
            # print(f"ルートロガーは既に設定済みです。レベルを {logging.getLevelName(self.log_level)} に調整しました。")
            pass


    def get_logger(self, name: str) -> logging.Logger:
        if name in self.loggers:
            return self.loggers[name]

        logger = logging.getLogger(name)
        # このマネージャーのログレベルを個々のロガーにも設定
        # (ルートロガーのレベルより詳細なログを個別に出せるようにするため)
        logger.setLevel(self.log_level)

        # ハンドラが重複しないように、このマネージャーでは個別のロガーにハンドラを追加しない
        # 親ロガー(最終的にはルートロガー)に伝播させることで、ルートのハンドラで処理
        logger.propagate = True

        self.loggers[name] = logger
        return logger

    def log_performance(self, operation: str, metrics: Dict[str, Any]) -> None:
        entry = metrics.copy()
        entry['timestamp'] = datetime.now().isoformat()
        entry['operation'] = operation
        self.performance_log.append(entry)

    def save_performance_log(self, filename: str = "performance_log.json") -> None:
        # パフォーマンスログは必ずJSONファイルに保存する
        if not self.performance_log:
            # self.get_logger(self.__class__.__name__).info("保存するパフォーマンスログがありません。")
            return

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.performance_log, f, indent=2, ensure_ascii=False)
            self.get_logger(self.__class__.__name__).info(f"パフォーマンスログを '{filename}' に保存しました。")
        except IOError as e:
            self.get_logger(self.__class__.__name__).error(f"パフォーマンスログ保存エラー ({filename}): {e}")
        except Exception as e: # その他の予期せぬエラー
            self.get_logger(self.__class__.__name__).error(f"パフォーマンスログ保存中に予期せぬエラー: {e}", exc_info=True)


# --- アプリケーション全体で共有するLoggerManagerインスタンス ---
# main.py のようなエントリーポイントで一度だけ初期化するのが理想
# ここではグローバルスコープに置くが、依存性注入(DI)の方が望ましい
APP_LOGGER_MANAGER = LoggerManager(log_level=logging.INFO, log_file="market_system.log")


class Config:
    """設定管理クラス (メイン設定ファイル config.json 用)"""

    # デフォルト設定のキー名はconfig.jsonのキー名と一致させる
    # (JSONのキーはキャメルケースやスネークケースが混在しているが、それをそのまま使う)
    DEFAULT_CONFIG = {
        "market_index_info": {
            "^GSPC": "S&P500指数",
            "^DJI": "NYダウ平均株価指数"
        },
        "csv_files": {
            "^GSPC": r"C:\Users\ds221k10159\Desktop\MymarketProject\Finance_Data\GSPC_ohlcv_5y_1d.csv",
            "^DJI": r"C:\Users\ds221k10159\Desktop\MymarketProject\Finance_Data\DJI_close_5y_1d.csv", # 末尾の不要なカンマを削除
            "^VIX": r"C:\Users\ds221k10159\Desktop\MymarketProject\Finance_Data\VIX_close_5y_1d.csv"
        }, # data_source_settings の前にカンマを追加
        "data_source_settings": {
            "fetch_period_years": "5y", # ここは元の config.json のキー名に合わせる
            "fetch_interval_days": "1d",
            "max_download_retries": 3,
            "download_retry_wait_seconds": 60,
            "wait_seconds_between_tickers": 15,
            "bulk_download_fail_wait_seconds": 180,
            "minimum_required_data_rows": 500,
            "data_backup_directory": "data_backup",
            "data_backup_max_age_days": 0
        },
        "feature_engineering_settings": { # 特徴量エンジニアリングに関する設定を追加
            "use_vix_feature": True,
            "use_dji_for_gspc_feature": True, # S&P500予測にダウ平均を使うか
            "technical_indicators_to_add": ["MA", "RSI", "MACD", "BB", "ATR", "CrossSignals"], # 使用するテクニカル指標リスト
            "ma_windows": [5, 20, 60, 120], # 旧 visualization_settings.moving_average_windows_days
            "rsi_window": 14,
            "bb_window": 20,
            "bb_std_dev": 2,
            "atr_window": 14,
            "macd_fast_period": 12,
            "macd_slow_period": 26,
            "macd_signal_period": 9,
        },
        "model_training_settings": {
            "random_seed": 42,
            "lstm_input_columns_for_gspc": ["^GSPC", "VIX", "^DJI"], # S&P500予測モデルの入力特徴量
            "train_test_split_ratio": 0.8,
            "hyperparameter_optimization_time_steps": 60, # Optuna時の固定タイムステップ
            "hyperparameter_optimization_epochs": 50,
            "hyperparameter_optimization_early_stopping_patience": 10,
            "model_training_early_stopping_patience": 15,
            "default_optimizer_algorithm": "adam", # AdamOptimizerOptionsを使用するので詳細設定可能に
            "default_learning_rate": 0.001,
            "default_loss_function": "mean_squared_error",
            "model_save_path_template": "models/model_{ticker}_{name}.keras", # .keras推奨
            "sp500_prediction_model_configs": { # 旧 sp500_model_definitions
                "nextday": {
                    "input_time_steps": 60, "prediction_horizon_days": 1, "lstm_layers_count": 1,
                    "use_optuna_params": True, "training_epochs": 50, "training_batch_size": 64
                },
                "short": {
                    "input_time_steps": 60, "prediction_horizon_days": 20, "lstm_layers_count": 1,
                    "lstm_units_per_layer": 64, "lstm_dropout_rate": 0.2,
                    "training_epochs": 75, "training_batch_size": 64
                },
                "long": {
                    "input_time_steps": 200, "prediction_horizon_days": 100, "lstm_layers_count": 2,
                    "lstm_units_per_layer": 48, "lstm_dropout_rate": 0.3,
                    "training_epochs": 100, "training_batch_size": 32
                }
            }
        },
        "hyperparameter_optimization_settings": {
            "default_optuna_trials": 50, # 元のコードに合わせて調整
            "load_best_hyperparameters_file": "best_lstm_params.json",
            "optuna_lstm_units_choices": [32, 64, 96, 128],
            "optuna_n_lstm_layers_range": [1, 2],
            "optuna_dropout_rate_range": [0.1, 0.5],
            "optuna_learning_rate_range": [1e-4, 1e-2],
            "optuna_batch_size_choices": [32, 64, 128]
        },
        "visualization_settings": {
            "plot_recent_days_count": 365,
            "plot_save_filename_template": "market_prediction_{ticker}.png",
            "plot_download_directory_candidates": ["Downloads", "ダウンロード", "."], # カレントディレクトリも候補に
            "correlation_matrix_features": ["Close", "^DJI", "VIX", "RSI", "MACD_diff"], # 旧 correlation_columns
            "plot_image_dpi": 300
        }
    }
    def __init__(self, config_file: str = "config.json", logger_manager: Optional[LoggerManager] = None):
        self.logger = (logger_manager or APP_LOGGER_MANAGER).get_logger(self.__class__.__name__)
        self.config_data = self._load_config(config_file)
        self.logger.info(f"メイン設定を '{config_file}' からロードしました。")

    def _deep_update(self, source: Dict, overrides: Dict) -> Dict:
        """ネストされた辞書を再帰的に更新するヘルパー関数"""
        for key, value in overrides.items():
            if isinstance(value, dict) and key in source and isinstance(source[key], dict):
                source[key] = self._deep_update(source[key], value)
            else:
                source[key] = value
        return source

    def _load_config(self, config_file: str) -> Dict[str, Any]:
        config_to_use = self.DEFAULT_CONFIG.copy() # まずデフォルトをコピー
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    # デフォルト設定をロードした設定で上書き (ネスト対応)
                    config_to_use = self._deep_update(config_to_use, loaded_config)
                    self.logger.debug(f"設定ファイル '{config_file}' の内容でデフォルト設定を更新しました。")
            else:
                self.logger.info(f"設定ファイル '{config_file}' が見つかりません。デフォルト設定を使用します。")
        except json.JSONDecodeError as e:
            self.logger.warning(f"設定ファイル '{config_file}' のJSONパースエラー: {e}。デフォルト設定を使用します。")
        except IOError as e:
            self.logger.warning(f"設定ファイル '{config_file}' の読み込みIOエラー: {e}。デフォルト設定を使用します。")
        except Exception as e:
            self.logger.error(f"設定ファイル '{config_file}' の読み込み中に予期せぬエラー: {e}。デフォルト設定を使用します。", exc_info=True)
        return config_to_use

    def get(self, key_path: str, default: Any = None) -> Any:
        """ドット区切りのキーパスで設定値を取得する。"""
        keys = key_path.split('.')
        value = self.config_data
        try:
            for key in keys:
                if isinstance(value, dict):
                    value = value[key]
                else: # パスの途中でdictでなくなった場合
                    # self.logger.debug(f"Configキー '{key_path}' の探索中、'{key}' の手前で非dict型に到達しました。")
                    return default
            return value
        except KeyError:
            # self.logger.debug(f"Configキー '{key_path}' が設定内に見つかりませんでした。デフォルト値 '{default}' を返します。")
            return default
        except Exception as e: # 予期せぬエラー
            self.logger.error(f"Configキー '{key_path}' の取得中に予期せぬエラー: {e}", exc_info=True)
            return default


class CSVDataFetcher:
    """CSVファイルから市場データを取得するクラス (元のコードベース)"""

    def __init__(self, config: 'Config', logger_manager: LoggerManager):
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.logger_manager = logger_manager # パフォーマンスログ用

        self.index_info = config.get("market_index_info", {}) # config.jsonのキー名に合わせる
        self.csv_files = config.get("csv_files", {}) # このキーはconfig.jsonにないので、別途追加が必要
        self.use_vix = config.get("feature_engineering_settings.use_vix_feature", True)
        self.use_dji_for_gspc = config.get("feature_engineering_settings.use_dji_for_gspc_feature", True)


    def _load_csv_file(self, file_path: str, ticker: str) -> Optional[pd.DataFrame]:
        self.logger.info(f"CSVファイル読み込み開始: {ticker} - '{file_path}'")
        try:
            if not os.path.exists(file_path):
                self.logger.error(f"CSVファイルが見つかりません: {file_path}")
                return None

            df = pd.read_csv(file_path)

            if 'Date' not in df.columns:
                self.logger.error(f"'Date'列が見つかりません: {file_path}")
                return None

            # 日付パースの改善
            try:
                # タイムゾーン情報が含まれる可能性のあるパターンを正規表現で除去
                # 例: '2023-01-01 00:00:00-05:00' -> '2023-01-01 00:00:00'
                df['Date'] = pd.to_datetime(df['Date'].astype(str).str.replace(r'[+-]\d{2}:\d{2}$', '', regex=True), errors='coerce')
                df.dropna(subset=['Date'], inplace=True) # パース失敗した行は削除
                df.set_index('Date', inplace=True)
            except Exception as e:
                self.logger.error(f"日付列の処理中にエラー ({file_path}): {e}")
                return None


            if df.empty:
                self.logger.warning(f"CSVファイルに有効なデータがありません (または日付パース後空になった): {file_path}")
                return None

            # 必要な列の確認 (例: Close)
            if 'Close' not in df.columns:
                self.logger.warning(f"'Close'列がありません: {file_path}。ティッカー: {ticker}")
                # Closeがない場合は処理が難しいのでNoneを返すか、ダミーを入れるか設計による
                # return None

            # データ型の変換 (数値であるべき列)
            for col in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce') # 数値に変換できないものはNaNに

            df.sort_index(inplace=True) # 日付でソート
            self.logger.info(f"CSVファイル読み込み完了: {ticker} - {len(df)}行 ({df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')})")
            return df

        except FileNotFoundError: # 上でチェック済みだが念のため
            self.logger.error(f"CSVファイルが見つかりません (FileNotFoundError): {file_path}")
            return None
        except pd.errors.EmptyDataError:
            self.logger.warning(f"CSVファイルが空です: {file_path}")
            return None
        except Exception as e:
            self.logger.error(f"CSVファイル読み込み中に予期せぬエラー ({file_path}): {e}", exc_info=True)
            return None


    def _prepare_gspc_data(self, gspc_df: pd.DataFrame, vix_df: Optional[pd.DataFrame] = None, dji_df: Optional[pd.DataFrame] = None) -> Optional[pd.DataFrame]:
        self.logger.debug(f"S&P500データ準備開始。元データ {len(gspc_df)}行")
        try:
            df = gspc_df.copy()
            if 'Close' not in df.columns: # 主要な価格データがない場合は処理困難
                self.logger.error("S&P500データに'Close'列がありません。")
                return None
            df['^GSPC'] = df['Close'] # S&P500自身の終値を別名でも保持

            if self.use_vix and vix_df is not None and 'Close' in vix_df.columns:
                vix_aligned = vix_df['Close'].reindex(df.index, method='ffill').fillna(method='bfill')
                df['VIX'] = vix_aligned
                self.logger.debug("VIXデータをS&P500データに追加しました。")

            if self.use_dji_for_gspc and dji_df is not None and 'Close' in dji_df.columns:
                dji_aligned = dji_df['Close'].reindex(df.index, method='ffill').fillna(method='bfill')
                df['^DJI'] = dji_aligned
                self.logger.debug("NYダウデータをS&P500データに追加しました。")

            # OHLCVデータが揃っているか確認 (テクニカル指標計算に影響)
            # 揃っていなくても処理は続けるが、警告は出す
            for col in ['Open', 'High', 'Low', 'Volume']:
                if col not in df.columns:
                    self.logger.warning(f"S&P500データに'{col}'列がありません。一部テクニカル指標に影響する可能性があります。")
                    df[col] = df['Close'] # ダミーとしてCloseで埋める (テクニカル指標ライブラリがエラーにならないように)


            df.dropna(subset=['^GSPC'], inplace=True) # ^GSPC列にNaNがある行は削除 (主要データなので)
            if df.empty:
                self.logger.warning("S&P500データ処理後にデータが空になりました。")
                return None

            self.logger.info(f"S&P500データ準備完了: {len(df)}行")
            return df
        except Exception as e:
            self.logger.error(f"S&P500データ準備エラー: {e}", exc_info=True)
            return None


    def _prepare_dji_data(self, dji_df: pd.DataFrame, vix_df: Optional[pd.DataFrame] = None) -> Optional[pd.DataFrame]:
        self.logger.debug(f"NYダウデータ準備開始。元データ {len(dji_df)}行")
        try:
            if 'Close' not in dji_df.columns:
                self.logger.error("NYダウデータに'Close'列がありません。")
                return None

            df = pd.DataFrame(index=dji_df.index)
            df['Close'] = dji_df['Close']
            df['^DJI'] = dji_df['Close']

            # NYダウは通常OHLVがない場合が多いので、Closeのみを主要データとする
            # テクニカル指標計算のためにダミー列を追加
            for col in ['Open', 'High', 'Low']:
                df[col] = df['Close']
            df['Volume'] = 0 # Volumeは通常ないので0で埋める

            if self.use_vix and vix_df is not None and 'Close' in vix_df.columns:
                vix_aligned = vix_df['Close'].reindex(df.index, method='ffill').fillna(method='bfill')
                df['VIX'] = vix_aligned
                self.logger.debug("VIXデータをNYダウデータに追加しました。")

            df.dropna(subset=['^DJI'], inplace=True)
            if df.empty:
                self.logger.warning("NYダウデータ処理後にデータが空になりました。")
                return None

            self.logger.info(f"NYダウデータ準備完了: {len(df)}行")
            return df
        except Exception as e:
            self.logger.error(f"NYダウデータ準備エラー: {e}", exc_info=True)
            return None


    def fetch_all_indexes(self) -> Dict[str, Dict[str, Any]]:
        """全ての指数データをCSVファイルから取得・準備する"""
        self.logger.info("CSVファイルからの市場データ読み込み処理開始...")
        start_time_fetch = datetime.now()
        fetched_data_store: Dict[str, Dict[str, Any]] = {}

        if not self.csv_files:
            self.logger.warning("設定に 'csv_files' が定義されていません。CSVデータ取得をスキップします。")
            return fetched_data_store
        if not self.index_info:
            self.logger.warning("設定に 'market_index_info' が定義されていません。主要な処理対象が不明です。")
            # return fetched_data_store # ここで処理を中断するかどうか

        # 1. 全CSVファイルを読み込み
        raw_csv_data: Dict[str, pd.DataFrame] = {}
        tickers_to_load = list(self.index_info.keys())
        if self.use_vix and '^VIX' not in tickers_to_load:
            tickers_to_load.append('^VIX') # VIXも対象に

        for ticker in tickers_to_load:
            file_path = self.csv_files.get(ticker)
            if file_path:
                df_loaded = self._load_csv_file(file_path, ticker)
                if df_loaded is not None and not df_loaded.empty:
                    raw_csv_data[ticker] = df_loaded
                else:
                    self.logger.warning(f"{ticker} のCSVデータ読み込みに失敗またはデータが空でした。")
            else:
                self.logger.warning(f"{ticker} のCSVファイルパスが設定 'csv_files' にありません。")


        # 2. 各主要指数のデータを準備
        vix_df_global = raw_csv_data.get('^VIX') if self.use_vix else None

        for ticker, name in self.index_info.items():
            self.logger.info(f"--- {name} ({ticker}) のデータ準備開始 ---")
            prepared_df: Optional[pd.DataFrame] = None

            if ticker == '^GSPC':
                gspc_base_df = raw_csv_data.get('^GSPC')
                if gspc_base_df is not None:
                    dji_base_df = raw_csv_data.get('^DJI') if self.use_dji_for_gspc else None
                    prepared_df = self._prepare_gspc_data(gspc_base_df, vix_df_global, dji_base_df)
                else:
                    self.logger.error(f"S&P500 ({ticker}) の元となるCSVデータが読み込まれていません。")
            elif ticker == '^DJI':
                dji_base_df = raw_csv_data.get('^DJI')
                if dji_base_df is not None:
                    prepared_df = self._prepare_dji_data(dji_base_df, vix_df_global)
                else:
                    self.logger.error(f"NYダウ ({ticker}) の元となるCSVデータが読み込まれていません。")
            # ... 他のティッカーの処理が必要な場合はここに追加 ...
            else:
                self.logger.warning(f"ティッカー '{ticker}' のデータ準備ロジックが実装されていません。スキップします。")
                continue

            if prepared_df is not None and not prepared_df.empty:
                fetched_data_store[ticker] = {
                    "df": prepared_df,
                    "ticker": ticker,
                    "name": name,
                    "scaler": None, # スケーラーはモデル学習時に設定
                    "scaled_data": None, # スケーリング済みデータも同様
                    "scaled_columns": None
                }
                self.logger.info(f"{name} ({ticker}) データ準備完了。期間: "
                                 f"{prepared_df.index.min().strftime('%Y-%m-%d')} to "
                                 f"{prepared_df.index.max().strftime('%Y-%m-%d')} ({len(prepared_df)}日分)")
            else:
                self.logger.error(f"{name} ({ticker}) のデータ準備に失敗しました。このティッカーは処理対象外となります。")

        duration_ms = (datetime.now() - start_time_fetch).total_seconds() * 1000
        self.logger_manager.log_performance(
            "fetch_data_from_csv",
            {
                "target_tickers_count": len(self.index_info),
                "successful_tickers_count": len(fetched_data_store),
                "duration_ms": round(duration_ms, 2),
                "loaded_tickers": list(raw_csv_data.keys()),
                "prepared_tickers": list(fetched_data_store.keys())
            }
        )

        if not fetched_data_store:
            self.logger.critical("有効な市場データをCSVから取得できませんでした。以降の処理に影響が出る可能性があります。")
        else:
            self.logger.info(f"CSVデータ準備完了。{len(fetched_data_store)}個の指数データを取得しました。")
        return fetched_data_store


class DataFetcher:
    """
    市場データ取得クラス（APIベース - レート制限対策強化版）
    このクラスは CurlSession が正しくエイリアス設定されていることを前提とする。
    """
    def __init__(self, config: 'Config', logger_manager: LoggerManager, session: Optional[Any] = None): # sessionの型はCurlSessionのエイリアス
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.logger_manager = logger_manager

        self.session = session
        if self.session is None and CurlSession is not None: # グローバルCurlSessionが利用可能ならそれを使う
            try:
                if CurlSession.__module__.startswith("curl_cffi"):
                    self.session = CurlSession(impersonate="chrome110")
                    self.logger.info("DataFetcher内でCurlSession (curl_cffi) を初期化しました。")
                else:
                    self.session = CurlSession()
                    self.logger.info("DataFetcher内でCurlSession (requests) を初期化しました。")
            except Exception as e:
                self.logger.error(f"DataFetcher内でのセッション初期化に失敗: {e}")
                self.session = None # やはり失敗
        elif self.session is None and CurlSession is None:
            self.logger.warning("DataFetcher: HTTPセッションが利用できません。APIベースのデータ取得は機能しません。")


        # 設定値の取得 (data_source_settings から)
        self.index_info = config.get("market_index_info", {})
        self.period = config.get("data_source_settings.fetch_period_years", "5y")
        self.interval = config.get("data_source_settings.fetch_interval_days", "1d")
        self.use_vix = config.get("feature_engineering_settings.use_vix_feature", True) # 特徴量設定から
        self.use_dji_for_gspc = config.get("feature_engineering_settings.use_dji_for_gspc_feature", True)

        self.max_retries = config.get("data_source_settings.max_download_retries", 3)
        self.retry_wait_seconds = config.get("data_source_settings.download_retry_wait_seconds", 60)
        self.inter_ticker_wait_seconds = config.get("data_source_settings.wait_seconds_between_tickers", 15)
        self.bulk_fail_wait_seconds = config.get("data_source_settings.bulk_download_fail_wait_seconds", 180)
        self.min_data_rows = config.get("data_source_settings.minimum_required_data_rows", 500)
        self.data_backup_dir = config.get("data_source_settings.data_backup_directory", "data_backup")
        self.backup_max_age_days = config.get("data_source_settings.data_backup_max_age_days", 0) # 0は無期限

        # yfinanceのセットアップ (もし使う場合)
        try:
            self.yf = install_and_import("yfinance")
            self.logger.info(f"yfinance version {self.yf.__version__} をロードしました。")
        except ImportError:
            self.yf = None
            self.logger.error("yfinanceライブラリのロードに失敗しました。APIベースのデータ取得は機能しません。")


    def _fetch_single_ticker_data_with_retry(self, ticker_symbol: str) -> Optional[pd.DataFrame]:
        """単一ティッカーのデータをリトライ付きで取得 (yfinanceを使用)"""
        if not self.yf:
            self.logger.error("yfinanceが利用不可なため、データ取得できません。")
            return None
        if not self.session: # yfinance自体はrequestsを使うが、カスタムセッションを渡せる場合がある
            self.logger.debug(f"{ticker_symbol}: HTTPセッションがないため、yfinanceのデフォルトセッションを使用します。")

        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"{ticker_symbol}: データ取得試行 {attempt + 1}/{self.max_retries + 1} (期間: {self.period}, 間隔: {self.interval})")
                # yf.Ticker(...).history(...) は内部でHTTPリクエストを行う
                # カスタムセッションを渡すオプションがあるか確認 (yfinanceのバージョンによる)
                # 現状のyfinanceでは直接requestsセッションを渡すAPIはない模様。
                # Proxy設定などはyf.set_proxy()で行う。
                # レート制限対策は主に時間をおくこと。
                ticker_obj = self.yf.Ticker(ticker_symbol) #, session=self.session if self.session else None) # session引数は公式にはない
                df = ticker_obj.history(period=self.period, interval=self.interval, auto_adjust=False) # auto_adjust=FalseでOHLCを保持

                if df.empty:
                    self.logger.warning(f"{ticker_symbol}: データ取得成功しましたが、DataFrameが空です。")
                    # 空でも成功として扱い、リトライしない場合もある。ここではリトライ対象とする。
                    if attempt < self.max_retries:
                        self.logger.info(f"{ticker_symbol}: {self.retry_wait_seconds}秒待機してリトライします。")
                        time.sleep(self.retry_wait_seconds)
                        continue
                    else: # 最終リトライでも空
                        return None # 空のDFを返すかNoneを返すか

                # タイムゾーン情報の除去 (Naiveなdatetimeに統一)
                if df.index.tz is not None:
                    df.index = df.index.tz_localize(None)

                df.dropna(subset=['Close'], inplace=True) # CloseがNaNの行は信頼性が低いので除去
                if len(df) < self.min_data_rows:
                    self.logger.warning(f"{ticker_symbol}: 取得データ行数 {len(df)} が最小要件 {self.min_data_rows} 未満です。")
                    if attempt < self.max_retries:
                        self.logger.info(f"{ticker_symbol}: {self.retry_wait_seconds}秒待機してリトライします。")
                        time.sleep(self.retry_wait_seconds)
                        continue
                    else: # 最終リトライでも不足
                         self.logger.error(f"{ticker_symbol}: データ行数不足で取得失敗。")
                         return None

                self.logger.info(f"{ticker_symbol}: データ取得成功 ({len(df)}行)。")
                return df

            except requests.exceptions.RequestException as re: # requestsライブラリ由来のエラー
                self.logger.error(f"{ticker_symbol}: データ取得中にネットワークエラー (試行 {attempt + 1}): {re}")
            except Exception as e: # yfinance内部エラーやその他の予期せぬエラー
                self.logger.error(f"{ticker_symbol}: データ取得中に予期せぬエラー (試行 {attempt + 1}): {e}", exc_info=True)

            if attempt < self.max_retries:
                self.logger.info(f"{ticker_symbol}: {self.retry_wait_seconds}秒待機してリトライします。")
                time.sleep(self.retry_wait_seconds)
            else:
                self.logger.error(f"{ticker_symbol}: 最大リトライ回数({self.max_retries})に達しました。データ取得失敗。")
        return None


    def _backup_data(self, df: pd.DataFrame, ticker_symbol: str) -> None:
        """DataFrameを指定されたディレクトリにCSVとしてバックアップする"""
        if not os.path.exists(self.data_backup_dir):
            try:
                os.makedirs(self.data_backup_dir)
                self.logger.info(f"バックアップディレクトリを作成しました: {self.data_backup_dir}")
            except OSError as e:
                self.logger.error(f"バックアップディレクトリ作成失敗: {e}. バックアップをスキップします。")
                return

        # ファイル名: ticker_YYYYMMDD_HHMMSS.csv
        filename = f"{ticker_symbol.replace('^','')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        filepath = os.path.join(self.data_backup_dir, filename)
        try:
            df.to_csv(filepath)
            self.logger.info(f"{ticker_symbol}のデータをバックアップしました: {filepath}")
        except IOError as e:
            self.logger.error(f"{ticker_symbol}のデータバックアップ失敗 ({filepath}): {e}")
        except Exception as e:
            self.logger.error(f"{ticker_symbol}のデータバックアップ中に予期せぬエラー: {e}", exc_info=True)


    def _cleanup_old_backups(self) -> None:
        """古いバックアップファイルを削除する"""
        if self.backup_max_age_days <= 0: # 0以下なら削除しない
            return
        if not os.path.exists(self.data_backup_dir):
            return

        self.logger.info(f"古いバックアップファイルのクリーンアップ開始 (保持期間: {self.backup_max_age_days}日)...")
        now = datetime.now()
        cleaned_count = 0
        try:
            for filename in os.listdir(self.data_backup_dir):
                filepath = os.path.join(self.data_backup_dir, filename)
                if os.path.isfile(filepath):
                    try:
                        # ファイル名から日付をパースする試み (例: ticker_YYYYMMDD_HHMMSS.csv)
                        parts = filename.split('_')
                        if len(parts) >= 2:
                            date_str = parts[-2] # YYYYMMDD
                            if len(date_str) == 8 and date_str.isdigit():
                                file_date = datetime.strptime(date_str, "%Y%m%d")
                                if (now - file_date).days > self.backup_max_age_days:
                                    os.remove(filepath)
                                    self.logger.debug(f"古いバックアップファイルを削除しました: {filepath}")
                                    cleaned_count += 1
                            else: # 日付形式でないファイルは最終更新日時で判断
                                file_mod_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                                if (now - file_mod_time).days > self.backup_max_age_days:
                                    os.remove(filepath)
                                    self.logger.debug(f"古いバックアップファイル(更新日時基準)を削除: {filepath}")
                                    cleaned_count += 1
                        else: # ファイル名形式が合わない場合も最終更新日時
                             file_mod_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                             if (now - file_mod_time).days > self.backup_max_age_days:
                                os.remove(filepath)
                                self.logger.debug(f"古いバックアップファイル(形式不一致、更新日時基準)を削除: {filepath}")
                                cleaned_count += 1
                    except ValueError: # 日付パース失敗
                        self.logger.debug(f"バックアップファイル名から日付をパースできませんでした: {filename}")
                    except OSError as e_remove:
                        self.logger.warning(f"バックアップファイル削除エラー ({filepath}): {e_remove}")
            if cleaned_count > 0:
                self.logger.info(f"{cleaned_count}個の古いバックアップファイルを削除しました。")
            else:
                self.logger.info("削除対象の古いバックアップファイルはありませんでした。")
        except Exception as e:
            self.logger.error(f"バックアップクリーンアップ中に予期せぬエラー: {e}", exc_info=True)

    def fetch_all_indexes(self) -> Dict[str, Dict[str, Any]]:
        """
        設定された全ての指数データをAPI経由で取得・準備する。
        CSVDataFetcher と同じ出力形式の辞書を返す。
        """
        self.logger.info("API経由での市場データ取得処理開始...")
        start_time_fetch_api = datetime.now()
        market_data_store_api: Dict[str, Dict[str, Any]] = {}

        if not self.yf:
            self.logger.critical("yfinanceが利用できないため、APIでのデータ取得は不可能です。")
            return market_data_store_api
        if not self.index_info:
            self.logger.warning("設定 'market_index_info' が空です。取得対象がありません。")
            return market_data_store_api

        tickers_to_process = list(self.index_info.keys())
        if self.use_vix and '^VIX' not in tickers_to_process:
            tickers_to_process.append('^VIX')

        raw_fetched_dfs: Dict[str, pd.DataFrame] = {}
        failed_tickers: List[str] = []

        # 1. 各ティッカーの生データを取得
        for i, ticker in enumerate(tickers_to_process):
            df_raw = self._fetch_single_ticker_data_with_retry(ticker)
            if df_raw is not None and not df_raw.empty:
                raw_fetched_dfs[ticker] = df_raw
                self._backup_data(df_raw, ticker) # 取得成功したらバックアップ
            else:
                self.logger.error(f"{ticker} のデータ取得に最終的に失敗しました。")
                failed_tickers.append(ticker)

            if i < len(tickers_to_process) - 1 and self.inter_ticker_wait_seconds > 0: # 最後のティッカー以外
                self.logger.debug(f"{self.inter_ticker_wait_seconds}秒待機 (次のティッカー取得前)...")
                time.sleep(self.inter_ticker_wait_seconds)

        if len(failed_tickers) == len(tickers_to_process) and tickers_to_process: # 全滅した場合
            self.logger.critical(f"全てのティッカー ({', '.join(failed_tickers)}) のデータ取得に失敗しました。{self.bulk_fail_wait_seconds}秒待機します。")
            time.sleep(self.bulk_fail_wait_seconds)
            # ここで処理を中断するか、空のデータを返すかは設計による
            return market_data_store_api


        # 2. CSVDataFetcherと同様のデータ準備ロジックを適用
        #    CSVDataFetcherのメソッドを再利用できるように、一時的なCSVFetcherインスタンスを作るか、
        #    準備ロジックを共通化する。ここでは簡易的にCSVFetcherの準備メソッドを呼び出す。
        temp_csv_fetcher = CSVDataFetcher(self.config, self.logger_manager) # logger_managerを渡す
        vix_df_global_api = raw_fetched_dfs.get('^VIX') if self.use_vix else None

        for ticker, name in self.index_info.items():
            if ticker in failed_tickers: # 生データ取得失敗したものはスキップ
                continue

            self.logger.info(f"--- {name} ({ticker}) のAPI取得データ準備開始 ---")
            prepared_df_api: Optional[pd.DataFrame] = None
            base_df = raw_fetched_dfs.get(ticker)

            if base_df is None or base_df.empty:
                self.logger.error(f"{name} ({ticker}) の元となるAPIデータがありません。")
                continue

            if ticker == '^GSPC':
                dji_base_df_api = raw_fetched_dfs.get('^DJI') if self.use_dji_for_gspc else None
                prepared_df_api = temp_csv_fetcher._prepare_gspc_data(base_df, vix_df_global_api, dji_base_df_api)
            elif ticker == '^DJI':
                prepared_df_api = temp_csv_fetcher._prepare_dji_data(base_df, vix_df_global_api)
            # ... 他のティッカー ...
            else:
                self.logger.warning(f"ティッカー '{ticker}' のAPIデータ準備ロジックが未実装。元データをそのまま使用します。")
                prepared_df_api = base_df # とりあえずそのまま格納

            if prepared_df_api is not None and not prepared_df_api.empty:
                market_data_store_api[ticker] = {
                    "df": prepared_df_api, "ticker": ticker, "name": name,
                    "scaler": None, "scaled_data": None, "scaled_columns": None
                }
                self.logger.info(f"{name} ({ticker}) APIデータ準備完了。 ({len(prepared_df_api)}日分)")
            else:
                self.logger.error(f"{name} ({ticker}) のAPIデータ準備に失敗。")

        self._cleanup_old_backups() # 古いバックアップを削除

        duration_ms_api = (datetime.now() - start_time_fetch_api).total_seconds() * 1000
        self.logger_manager.log_performance(
            "fetch_data_from_api",
            {
                "target_tickers_count": len(self.index_info),
                "successful_tickers_count": len(market_data_store_api),
                "duration_ms": round(duration_ms_api, 2),
                "fetched_tickers_raw": list(raw_fetched_dfs.keys()),
                "prepared_tickers": list(market_data_store_api.keys()),
                "failed_tickers_raw": failed_tickers
            }
        )

        if not market_data_store_api:
            self.logger.critical("API経由で有効な市場データを取得できませんでした。")
        else:
            self.logger.info(f"APIデータ準備完了。{len(market_data_store_api)}個の指数データを取得。")
        return market_data_store_api


class FeatureEngineering:
    """特徴量エンジニアリングクラス"""

    def __init__(self, config: 'Config', logger_manager: LoggerManager):
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        # 設定からテクニカル指標のパラメータを取得
        self.fe_settings = config.get("feature_engineering_settings", {})
        self.indicators_to_add = self.fe_settings.get("technical_indicators_to_add", [])
        self.ma_windows = self.fe_settings.get("ma_windows", [5, 20, 60, 120])
        self.rsi_window = self.fe_settings.get("rsi_window", 14)
        self.bb_window = self.fe_settings.get("bb_window", 20)
        self.bb_std_dev = self.fe_settings.get("bb_std_dev", 2)
        self.atr_window = self.fe_settings.get("atr_window", 14)
        self.macd_fast = self.fe_settings.get("macd_fast_period", 12)
        self.macd_slow = self.fe_settings.get("macd_slow_period", 26)
        self.macd_sign = self.fe_settings.get("macd_signal_period", 9)

        self.rsi_oversold = 30 # 固定値またはconfigから
        self.rsi_overbought = 70

    def _ensure_required_columns(self, df: pd.DataFrame, required_cols: List[str]) -> bool:
        """DataFrameに必要な列が存在するか確認し、なければ警告"""
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            self.logger.warning(f"テクニカル指標計算に必要な列が不足しています: {missing_cols}。該当指標の計算をスキップします。")
            return False
        return True

    def _add_moving_averages(self, df: pd.DataFrame) -> None:
        if "MA" not in self.indicators_to_add or not self._ensure_required_columns(df, ["Close"]):
            return
        close = df["Close"]
        for window in self.ma_windows:
            if len(close) >= window:
                df[f"MA{window}"] = ta.trend.sma_indicator(close, window=window, fillna=False) # fillna=Falseでtaライブラリのデフォルト挙動
            else:
                df[f"MA{window}"] = np.nan
                self.logger.debug(f"MA{window} 計算スキップ: データ長 ({len(close)}) < ウィンドウ ({window})")

    def _add_cross_signals(self, df: pd.DataFrame) -> None:
        if "CrossSignals" not in self.indicators_to_add or not self._ensure_required_columns(df, ["MA5", "MA20"]): # MA5, MA20を仮定
            # MA5, MA20がなければ、ma_windowsの最初の2つを使うなどのロジックも可能
            # self.logger.debug("MA5またはMA20が存在しないため、クロスシグナル計算をスキップします。")
            return

        # 短期MAと中期MAを特定 (ma_windowsから)
        if len(self.ma_windows) >= 2:
            short_ma_col = f"MA{self.ma_windows[0]}"
            mid_ma_col = f"MA{self.ma_windows[1]}"
            if short_ma_col in df.columns and mid_ma_col in df.columns:
                df["golden_cross"] = (df[short_ma_col] > df[mid_ma_col]) & (df[short_ma_col].shift(1) <= df[mid_ma_col].shift(1))
                df["death_cross"] = (df[short_ma_col] < df[mid_ma_col]) & (df[short_ma_col].shift(1) >= df[mid_ma_col].shift(1))
            else:
                self.logger.debug(f"{short_ma_col} または {mid_ma_col} がDataFrameにないため、クロスシグナル計算をスキップ。")
                df["golden_cross"] = False
                df["death_cross"] = False
        else:
            df["golden_cross"] = False
            df["death_cross"] = False


    def _add_rsi(self, df: pd.DataFrame) -> None:
        if "RSI" not in self.indicators_to_add or not self._ensure_required_columns(df, ["Close"]):
            return
        close = df["Close"]
        if len(close) >= self.rsi_window:
            rsi_indicator = ta.momentum.RSIIndicator(close, window=self.rsi_window, fillna=False)
            df["RSI"] = rsi_indicator.rsi()
            # クロスシグナル (価格が閾値をクロスした瞬間)
            df["RSI_buy_signal"] = (df["RSI"] < self.rsi_oversold) & (df["RSI"].shift(1) >= self.rsi_oversold)
            df["RSI_sell_signal"] = (df["RSI"] > self.rsi_overbought) & (df["RSI"].shift(1) <= self.rsi_overbought)
            # 状態シグナル (現在閾値を超えているか)
            df["RSI_oversold"] = df["RSI"] < self.rsi_oversold
            df["RSI_overbought"] = df["RSI"] > self.rsi_overbought
        else:
            for col in ["RSI", "RSI_buy_signal", "RSI_sell_signal", "RSI_oversold", "RSI_overbought"]: df[col] = np.nan
            self.logger.debug(f"RSI 計算スキップ: データ長 ({len(close)}) < ウィンドウ ({self.rsi_window})")


    def _add_macd(self, df: pd.DataFrame) -> None:
        if "MACD" not in self.indicators_to_add or not self._ensure_required_columns(df, ["Close"]):
            return
        close = df["Close"]
        min_len_macd = max(self.macd_fast, self.macd_slow, self.macd_sign) # MACD計算に必要な最小期間
        if len(close) >= min_len_macd:
            macd_indicator = ta.trend.MACD(close, window_slow=self.macd_slow, window_fast=self.macd_fast, window_sign=self.macd_sign, fillna=False)
            df["MACD"] = macd_indicator.macd()
            df["MACD_signal"] = macd_indicator.macd_signal()
            df["MACD_diff"] = macd_indicator.macd_diff() # ヒストグラム
            # MACDクロスシグナル
            df["MACD_buy_signal"] = (df["MACD"] > df["MACD_signal"]) & (df["MACD"].shift(1) <= df["MACD_signal"].shift(1))
            df["MACD_sell_signal"] = (df["MACD"] < df["MACD_signal"]) & (df["MACD"].shift(1) >= df["MACD_signal"].shift(1))
        else:
            for col in ["MACD", "MACD_signal", "MACD_diff", "MACD_buy_signal", "MACD_sell_signal"]: df[col] = np.nan
            self.logger.debug(f"MACD 計算スキップ: データ長 ({len(close)}) < 最小必要期間 ({min_len_macd})")


    def _add_bollinger_bands(self, df: pd.DataFrame) -> None:
        if "BB" not in self.indicators_to_add or not self._ensure_required_columns(df, ["Close"]):
            return
        close = df["Close"]
        if len(close) >= self.bb_window:
            bollinger_indicator = ta.volatility.BollingerBands(close, window=self.bb_window, window_dev=self.bb_std_dev, fillna=False)
            df["BB_High"] = bollinger_indicator.bollinger_hband()
            df["BB_Mid"] = bollinger_indicator.bollinger_mavg()
            df["BB_Low"] = bollinger_indicator.bollinger_lband()
            df["BB_Width"] = bollinger_indicator.bollinger_wband() # バンド幅
            df["BB_Percent"] = bollinger_indicator.bollinger_pband() # %B
            # BBクロス/タッチシグナル
            df["BB_buy_signal"] = (close < df["BB_Low"]) & (close.shift(1) >= df["BB_Low"].shift(1)) # 下抜けクロス
            df["BB_sell_signal"] = (close > df["BB_High"]) & (close.shift(1) <= df["BB_High"].shift(1)) # 上抜けクロス
        else:
            for col in ["BB_High", "BB_Mid", "BB_Low", "BB_Width", "BB_Percent", "BB_buy_signal", "BB_sell_signal"]: df[col] = np.nan
            self.logger.debug(f"BB 計算スキップ: データ長 ({len(close)}) < ウィンドウ ({self.bb_window})")


    def _add_atr(self, df: pd.DataFrame) -> None:
        if "ATR" not in self.indicators_to_add or not self._ensure_required_columns(df, ["High", "Low", "Close"]):
            return
        if len(df) >= self.atr_window: # ATRはDataFrame全体の長さ
            atr_indicator = ta.volatility.AverageTrueRange(
                high=df["High"], low=df["Low"], close=df["Close"],
                window=self.atr_window, fillna=False
            )
            df["ATR"] = atr_indicator.average_true_range()
        else:
            df["ATR"] = np.nan
            self.logger.debug(f"ATR 計算スキップ: データ長 ({len(df)}) < ウィンドウ ({self.atr_window})")


    def add_technical_indicators(self, market_data: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        self.logger.info("テクニカル指標の計算処理開始...")
        for ticker_symbol, data_entry in market_data.items():
            df = data_entry.get("df")
            if df is None or df.empty:
                self.logger.warning(f"{ticker_symbol}: DataFrameが存在しないか空のため、テクニカル指標計算をスキップ。")
                continue

            self.logger.info(f"--- {ticker_symbol}: テクニカル指標計算開始 ---")
            df_with_ta = df.copy() # 元のDataFrameを変更しない

            # 各指標計算メソッドを呼び出し
            self._add_moving_averages(df_with_ta)
            self._add_cross_signals(df_with_ta) # MA計算後に呼び出す
            self._add_rsi(df_with_ta)
            self._add_macd(df_with_ta)
            self._add_bollinger_bands(df_with_ta)
            self._add_atr(df_with_ta)
            # 他の指標も同様に追加

            # fillna(method='bfill') で先頭のNaNを後方の値で埋める (オプション)
            # LSTM入力前には結局dropnaするので、ここでは積極的なNaN埋めは必須ではない
            # df_with_ta.fillna(method='bfill', inplace=True)

            market_data[ticker_symbol]["df"] = df_with_ta
            self.logger.info(f"{ticker_symbol}: テクニカル指標計算完了。DataFrame 行数: {len(df_with_ta)}")

        self.logger.info("全ティッカーのテクニカル指標計算処理完了。")
        return market_data


class LSTMModel:
    """LSTMモデル訓練と予測クラス"""

    def __init__(self, config: 'Config', logger_manager: LoggerManager):
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.logger_manager = logger_manager # パフォーマンスログ用

        self.model_settings = config.get("model_training_settings", {})
        self.opt_settings = config.get("hyperparameter_optimization_settings", {})

        self.seed = self.model_settings.get("random_seed", 42)
        self.set_random_seed()

        self.best_params: Optional[Dict[str, Any]] = None
        self.hyperparams_file = self.opt_settings.get("load_best_hyperparameters_file", "best_lstm_params.json")


    def set_random_seed(self):
        """各種ライブラリの乱数シードを設定する"""
        os.environ['PYTHONHASHSEED'] = str(self.seed)
        os.environ['TF_DETERMINISTIC_OPS'] = '1' # TFの決定論的動作 (可能な範囲で)
        random.seed(self.seed)
        np.random.seed(self.seed)
        tf.random.set_seed(self.seed)
        if optuna: # optunaがインポートされていれば、その乱数シードも設定
            # TPESamplerのseedはStudy作成時に指定
            pass
        self.logger.debug(f"乱数シードを {self.seed} に設定しました。")


    def load_best_params(self, filepath: Optional[str] = None) -> bool:
        load_path = filepath or self.hyperparams_file
        self.logger.info(f"最適化済みハイパーパラメータを '{load_path}' からロード試行...")
        try:
            with open(load_path, "r", encoding="utf-8") as f:
                self.best_params = json.load(f)
            self.logger.info(f"ハイパーパラメータをロードしました: {self.best_pLarams}")
            return True
        except FileNotFoundError:
            self.logger.info(f"ハイパーパラメータファイル '{load_path}' が見つかりません。")
            self.best_params = None
            return False
        except json.JSONDecodeError as e:
            self.logger.error(f"ハイパーパラメータファイル '{load_path}' のJSONパースエラー: {e}")
            self.best_params = None
            return False
        except Exception as e:
            self.logger.error(f"ハイパーパラメータファイル '{load_path}' のロード中に予期せぬエラー: {e}", exc_info=True)
            self.best_params = None
            return False

    def save_best_params(self, params: Dict[str, Any], filepath: Optional[str] = None):
        save_path = filepath or self.hyperparams_file
        self.logger.info(f"最適ハイパーパラメータを '{save_path}' に保存試行...")
        try:
            # 保存先ディレクトリが存在しない場合は作成
            save_dir = os.path.dirname(save_path)
            if save_dir and not os.path.exists(save_dir):
                os.makedirs(save_dir)
                self.logger.info(f"保存先ディレクトリを作成しました: {save_dir}")

            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(params, f, indent=2, ensure_ascii=False)
            self.logger.info(f"最適ハイパーパラメータを '{save_path}' に保存しました。")
        except IOError as e:
            self.logger.error(f"ハイパーパラメータのファイル保存IOエラー ({save_path}): {e}")
        except Exception as e:
            self.logger.error(f"ハイパーパラメータの保存中に予期せぬエラー ({save_path}): {e}", exc_info=True)


    def _prepare_data_for_lstm(
        self, df: pd.DataFrame, ticker_symbol: str
    ) -> Tuple[Optional[np.ndarray], Optional[MinMaxScaler], Optional[List[str]], Optional[pd.Index]]:
        """DataFrameからLSTMモデル用のスケーリング済み多変量データを準備する。スケーリングに使ったインデックスも返す。"""
        self.logger.debug(f"{ticker_symbol}: LSTM用データ準備開始...")

        # スケーリング対象列 (S&P500の場合のみ特別扱い、他はCloseのみなど柔軟に)
        if ticker_symbol == "^GSPC":
            potential_cols = self.model_settings.get("lstm_input_columns_for_gspc", ["^GSPC", "VIX", "^DJI"])
        else: # 他のティッカーは自身の終値のみ、または設定で指定
            potential_cols = [ticker_symbol] # configで指定できるようにしても良い

        cols_for_scaling = [col for col in potential_cols if col in df.columns and df[col].isnull().sum() < len(df)] # 全てNaNの列は除外
        if not cols_for_scaling or ticker_symbol not in cols_for_scaling: # ticker_symbol自体が含まれているか
             # S&P500以外でティッカー名が 'Close' と異なる場合、ticker_symbolの代わりに 'Close' を探す
            if ticker_symbol not in cols_for_scaling and 'Close' in df.columns:
                cols_for_scaling = ['Close'] # 主対象をCloseとする
                if 'VIX' in df.columns and self.config.get("feature_engineering_settings.use_vix_feature"):
                    cols_for_scaling.append('VIX')
            else:
                self.logger.error(f"{ticker_symbol}: LSTM用データ準備エラー。スケーリング対象の主列が見つかりません。候補: {potential_cols}, 存在列: {list(df.columns)}")
                return None, None, None, None

        # 欠損値処理: LSTM入力前には欠損がない状態にする
        # ここではdropnaするが、より高度な補完処理も検討可能 (例: ffill後bfill)
        # df_processed = df[cols_for_scaling].fillna(method='ffill').fillna(method='bfill') # 先に補完
        # data_to_scale = df_processed.dropna() # それでも残るNaNがあれば削除
        data_to_scale = df[cols_for_scaling].dropna() # シンプルにdropna

        if data_to_scale.empty:
            self.logger.error(f"{ticker_symbol}: LSTM用データ準備エラー。dropna後データが空になりました。対象列: {cols_for_scaling}")
            return None, None, None, None

        original_index = data_to_scale.index # スケーリングに使用したデータのインデックスを保持

        scaler = MinMaxScaler(feature_range=(0, 1))
        try:
            scaled_data = scaler.fit_transform(data_to_scale)
        except Exception as e:
            self.logger.error(f"{ticker_symbol}: データスケーリング中にエラー: {e}", exc_info=True)
            return None, None, None, None

        self.logger.info(f"{ticker_symbol}: LSTM用データ準備完了。スケーリング対象列: {cols_for_scaling}, スケーリング後形状: {scaled_data.shape}")
        return scaled_data, scaler, cols_for_scaling, original_index


    def create_multivariate_dataset(
        self, data: np.ndarray, time_step: int, predict_step: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        X, y = [], []
        if data.ndim == 1: data = data.reshape(-1, 1)

        if len(data) <= time_step + predict_step -1: # データが足りない場合 (等号を含む)
            self.logger.warning(f"データ長({len(data)})がtime_step({time_step}) + predict_step({predict_step})に対して不足。データセット作成不可。")
            return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

        for i in range(len(data) - time_step - predict_step + 1):
            X.append(data[i:(i + time_step), :])
            y.append(data[i + time_step : i + time_step + predict_step, 0]) # 予測対象は常に最初の特徴量
        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)


    def _split_train_test(
        self, X: np.ndarray, y: np.ndarray, train_ratio: float
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        if len(X) == 0: return np.array([]), np.array([]), np.array([]), np.array([])
        
        train_size = int(len(X) * train_ratio)
        # 訓練データ、テストデータが最低1サンプルは存在するように調整
        if train_size <= 0 and len(X) > 1: train_size = 1 # 少なくとも1つは訓練
        if train_size >= len(X) and len(X) > 1: train_size = len(X) - 1 # 少なくとも1つはテスト

        if train_size == 0 and len(X) > 0 : # データが1つしかない場合など
             self.logger.warning(f"データ数が非常に少ないため({len(X)}サンプル)、train_sizeが0。全てを訓練/テストデータとします。")
             return X, X, y, y # 訓練とテストを同じにする（評価には不適切だが実行は可能）
        if train_size == len(X):
             self.logger.warning(f"データ数が非常に少ないため({len(X)}サンプル)、全て訓練データ。テストデータを複製します。")
             return X, X, y, y

        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        return X_train, X_test, y_train, y_test


    def _build_model(
        self, input_shape: Tuple[int, int], lstm_units: int, dropout_rate: float,
        predict_step: int, n_lstm_layers: int = 1, learning_rate: Optional[float] = None,
    ) -> tf.keras.Model:
        tf.keras.backend.clear_session() # モデル構築前にセッションクリア
        self.set_random_seed() # 再現性のためにここでもシード設定

        model = Sequential()
        for i in range(n_lstm_layers):
            return_sequences = True if i < n_lstm_layers - 1 else False # 最後のLSTM層以外はTrue
            if i == 0: # 最初の層のみinput_shapeを指定
                model.add(LSTM(lstm_units, return_sequences=return_sequences, input_shape=input_shape))
            else:
                model.add(LSTM(lstm_units, return_sequences=return_sequences))
            model.add(Dropout(dropout_rate))

        model.add(Dense(predict_step)) # 出力層: 予測ステップ数分のユニット

        optimizer_name = self.model_settings.get("default_optimizer_algorithm", "adam").lower()
        final_learning_rate = learning_rate if learning_rate is not None else self.model_settings.get("default_learning_rate", 0.001)

        if optimizer_name == "adam":
            optimizer = KerasAdam(learning_rate=final_learning_rate) # tensorflow.keras.optimizers.Adam を使用
        else: # 他のオプティマイザ (例: RMSprop)
            self.logger.warning(f"オプティマイザ '{optimizer_name}' はAdam以外未実装です。Adam (lr={final_learning_rate}) を使用します。")
            optimizer = KerasAdam(learning_rate=final_learning_rate)

        loss_function = self.model_settings.get("default_loss_function", 'mean_squared_error')
        model.compile(optimizer=optimizer, loss=loss_function)
        # self.logger.debug(f"モデル構築完了: LSTM層={n_lstm_layers}, Units={lstm_units}, Dropout={dropout_rate}, LR={final_learning_rate}, Loss={loss_function}")
        # model.summary(print_fn=self.logger.debug) # ログにサマリ出力
        return model


    def _inverse_transform_predictions(
        self, scaler: MinMaxScaler, predictions: np.ndarray, num_scaled_features: int
    ) -> np.ndarray:
        if predictions.ndim == 1: predictions = predictions.reshape(-1, 1)
        if predictions.shape[1] > 1 and predictions.shape[1] != num_scaled_features :
             # 複数ステップ予測の場合、predictionsは(samples, predict_steps)
             # 逆変換は各ステップごとに行う必要がある
             # ここでは、予測対象は常に最初の特徴量であると仮定している
             inverted_preds_list = []
             for step_idx in range(predictions.shape[1]): # 各予測ステップに対して
                dummy_step_pred = np.zeros((predictions.shape[0], num_scaled_features))
                dummy_step_pred[:, 0] = predictions[:, step_idx]
                inverted_step = scaler.inverse_transform(dummy_step_pred)[:, 0]
                inverted_preds_list.append(inverted_step)
             return np.array(inverted_preds_list).T # (samples, predict_steps) に転置

        # 1ステップ予測または1特徴量予測の場合
        dummy_predictions = np.zeros((predictions.shape[0], num_scaled_features))
        dummy_predictions[:, 0] = predictions[:, 0] # 予測値を最初の列に配置
        try:
            original_scale_predictions_full = scaler.inverse_transform(dummy_predictions)
            return original_scale_predictions_full[:, 0].reshape(-1, 1) # 最初の列のみ返す
        except ValueError as ve: # スケーラーの特徴量数と合わない場合など
            self.logger.error(f"逆変換エラー: {ve}. Scaler features: {getattr(scaler, 'n_features_in_', 'N/A')}, Preds shape for dummy: {dummy_predictions.shape}")
            return predictions # エラー時はスケールされた値をそのまま返す
        except Exception as e:
            self.logger.error(f"予期せぬ逆変換エラー: {e}", exc_info=True)
            return predictions

    def optimize_hyperparameters(
        self, market_data_dict: Dict[str, Dict[str, Any]], target_ticker: str = "^GSPC",
        n_trials: Optional[int] = None
    ) -> Optional[Dict[str, Any]]:
        self.logger.info(f"Optunaによるハイパーパラメータ最適化開始 (対象: {target_ticker})")
        start_time_opt = datetime.now()

        n_trials_actual = n_trials if n_trials is not None else self.opt_settings.get("default_optuna_trials", 50)
        if n_trials_actual <= 0:
            self.logger.warning("Optuna試行回数が0以下です。最適化をスキップします。")
            return self.best_params # 既存のパラメータを返すか、None

        df_target = market_data_dict.get(target_ticker, {}).get("df")
        if df_target is None or df_target.empty:
            self.logger.error(f"最適化対象 {target_ticker} のDataFrameが見つからないか空です。最適化中止。")
            return None

        scaled_data, scaler, scaled_cols, _ = self._prepare_data_for_lstm(df_target, target_ticker)
        if scaled_data is None or scaler is None or not scaled_cols:
            self.logger.error(f"{target_ticker}: LSTM用データ準備失敗。最適化中止。")
            return None

        time_step_opt = self.model_settings.get("hyperparameter_optimization_time_steps", 60)
        predict_step_opt = 1 # 翌日予測で最適化 (固定)

        X_all, y_all = self.create_multivariate_dataset(scaled_data, time_step_opt, predict_step_opt)
        if X_all.shape[0] == 0:
            self.logger.error(f"{target_ticker}: Optuna用データセット作成失敗。データ不足の可能性。最適化中止。")
            return None

        train_ratio_opt = self.model_settings.get("train_test_split_ratio", 0.8)
        X_train_opt, X_val_opt, y_train_opt, y_val_opt = self._split_train_test(X_all, y_all, train_ratio=train_ratio_opt)
        if X_train_opt.shape[0] == 0 or X_val_opt.shape[0] == 0:
            self.logger.error(f"{target_ticker}: Optuna用訓練/検証データが空。データ不足の可能性。最適化中止。")
            return None

        def objective(trial: optuna.Trial) -> float:
            # ハイパーパラメータの提案範囲 (configから取得)
            lstm_units = trial.suggest_categorical('lstm_units', self.opt_settings.get("optuna_lstm_units_choices", [64, 128]))
            n_lstm_layers = trial.suggest_int('n_lstm_layers', *self.opt_settings.get("optuna_n_lstm_layers_range", [1,2]))
            dropout_rate = trial.suggest_float('dropout_rate', *self.opt_settings.get("optuna_dropout_rate_range", [0.1, 0.5]))
            learning_rate = trial.suggest_float('learning_rate', *self.opt_settings.get("optuna_learning_rate_range", [1e-4, 1e-2]), log=True)
            batch_size = trial.suggest_categorical('batch_size', self.opt_settings.get("optuna_batch_size_choices", [32, 64]))
            epochs_opt = self.model_settings.get("hyperparameter_optimization_epochs", 50)

            model = self._build_model(
                input_shape=(X_train_opt.shape[1], X_train_opt.shape[2]),
                lstm_units=lstm_units, n_lstm_layers=n_lstm_layers, dropout_rate=dropout_rate,
                predict_step=predict_step_opt, learning_rate=learning_rate,
            )
            early_stop_patience = self.model_settings.get("hyperparameter_optimization_early_stopping_patience", 10)
            early_stop = EarlyStopping(monitor='val_loss', patience=early_stop_patience, restore_best_weights=True, verbose=0)

            history = model.fit(
                X_train_opt, y_train_opt, epochs=epochs_opt, batch_size=batch_size,
                validation_data=(X_val_opt, y_val_opt), callbacks=[early_stop], verbose=0
            )
            val_loss = min(history.history.get('val_loss', [float('inf')]))
            del model, history; gc.collect() # メモリ解放
            return val_loss

        sampler = optuna.samplers.TPESampler(seed=self.seed) # シード固定
        study = optuna.create_study(direction="minimize", sampler=sampler)
        try:
            # n_jobs > 1 はTensorFlow/Kerasと競合することがあるので注意。デフォルト1。
            study.optimize(objective, n_trials=n_trials_actual, n_jobs=1, show_progress_bar=True)
        except Exception as e:
            self.logger.error(f"Optuna最適化中にエラー: {e}", exc_info=True)
            return self.best_params # 既存のパラメータを返す

        self.best_params = study.best_params
        self.logger.info(f"Optuna最適化完了。最適パラメータ ({target_ticker}): {self.best_params}, 最小検証損失: {study.best_value:.6f}")
        self.save_best_params(self.best_params)

        duration_ms_opt = (datetime.now() - start_time_opt).total_seconds() * 1000
        self.logger_manager.log_performance(
            f"hyperparameter_optimization_{target_ticker}",
            {
                "n_trials_run": n_trials_actual,
                "best_params_found": study.best_params,
                "best_value_val_loss": study.best_value,
                "duration_ms": round(duration_ms_opt, 2)
            }
        )
        return self.best_params


    def train_models_for_sp500(self, market_data_dict: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        self.logger.info("S&P500 LSTMモデル群の学習処理開始...")
        start_time_train_all = datetime.now()
        target_ticker = "^GSPC"
        trained_models_output: Dict[str, Dict[str, Any]] = {}

        df_sp500 = market_data_dict.get(target_ticker, {}).get("df")
        if df_sp500 is None or df_sp500.empty:
            self.logger.error(f"{target_ticker} のDataFrameが見つからないか空です。モデル学習中止。")
            return trained_models_output

        scaled_data_sp500, scaler_sp500, scaled_cols_sp500, original_indices_sp500 = self._prepare_data_for_lstm(df_sp500, target_ticker)
        if scaled_data_sp500 is None or scaler_sp500 is None or not scaled_cols_sp500 or original_indices_sp500 is None:
            self.logger.error(f"{target_ticker}: LSTM用データ準備失敗。モデル学習中止。")
            return trained_models_output

        # market_data_dictにスケーラー情報を保存 (可視化やアドバイザーで使うため)
        market_data_dict[target_ticker]["scaler"] = scaler_sp500
        market_data_dict[target_ticker]["scaled_columns"] = scaled_cols_sp500
        market_data_dict[target_ticker]["scaled_data_index"] = original_indices_sp500


        model_definitions = self.model_settings.get("sp500_prediction_model_configs", {})
        if not model_definitions:
            self.logger.warning("S&P500モデル定義 (sp500_prediction_model_configs) が設定にありません。学習スキップ。")
            return trained_models_output

        for model_name, params_def in model_definitions.items():
            self.logger.info(f"--- '{model_name}'モデル ({target_ticker}) の学習開始 ---")
            start_time_train_model = datetime.now()

            time_step = params_def["input_time_steps"]
            predict_step = params_def["prediction_horizon_days"]

            X_all, y_all = self.create_multivariate_dataset(scaled_data_sp500, time_step, predict_step)
            if X_all.shape[0] == 0:
                self.logger.error(f"{model_name} ({target_ticker}): データセット作成失敗。スキップ。")
                continue

            train_ratio = self.model_settings.get("train_test_split_ratio", 0.8)
            X_train, X_test, y_train, y_test = self._split_train_test(X_all, y_all, train_ratio=train_ratio)
            if X_train.shape[0] == 0 or X_test.shape[0] == 0:
                 self.logger.error(f"{model_name} ({target_ticker}): 訓練/テストデータ空。スキップ。")
                 continue

            # パラメータ設定 (Optuna優先)
            cfg_lstm_units = params_def.get("lstm_units_per_layer")
            cfg_dropout = params_def.get("lstm_dropout_rate")
            cfg_lr = params_def.get("learning_rate") # モデル定義になければNone
            cfg_batch_size = params_def.get("training_batch_size")
            cfg_epochs = params_def.get("training_epochs")
            cfg_n_layers = params_def.get("lstm_layers_count",1)

            if params_def.get("use_optuna_params", False) and self.best_params:
                self.logger.info(f"'{model_name}'モデルにOptuna最適化パラメータを使用: {self.best_params}")
                final_lstm_units = self.best_params.get('lstm_units', cfg_lstm_units)
                final_dropout = self.best_params.get('dropout_rate', cfg_dropout)
                final_lr = self.best_params.get('learning_rate', cfg_lr) # OptunaでLRも最適化した場合
                final_batch_size = self.best_params.get('batch_size', cfg_batch_size)
                # epochsはOptuna対象外ならモデル定義の値を使用
                final_n_layers = self.best_params.get('n_lstm_layers', cfg_n_layers)
            else: # Optuna不使用またはパラメータなし
                final_lstm_units, final_dropout, final_lr, final_batch_size, final_n_layers = \
                    cfg_lstm_units, cfg_dropout, cfg_lr, cfg_batch_size, cfg_n_layers

            if not all([final_lstm_units, final_dropout is not None, final_batch_size, cfg_epochs, final_n_layers]): # LRはNone許容
                self.logger.error(f"'{model_name}'パラメータ不足。スキップ。Units:{final_lstm_units}, Dropout:{final_dropout}, Batch:{final_batch_size}, Epochs:{cfg_epochs}, Layers:{final_n_layers}")
                continue

            model = self._build_model(
                input_shape=(X_train.shape[1], X_train.shape[2]),
                lstm_units=final_lstm_units, n_lstm_layers=final_n_layers, dropout_rate=final_dropout,
                predict_step=predict_step, learning_rate=final_lr
            )
            early_stop_patience_train = self.model_settings.get("model_training_early_stopping_patience", 15)
            early_stop = EarlyStopping(monitor='val_loss', patience=early_stop_patience_train, restore_best_weights=True, verbose=1)

            history = model.fit(
                X_train, y_train, epochs=cfg_epochs, batch_size=final_batch_size,
                validation_data=(X_test, y_test), callbacks=[early_stop], verbose=1
            )
            training_duration_model_ms = (datetime.now() - start_time_train_model).total_seconds() * 1000

            y_pred_scaled_test = model.predict(X_test)
            # 逆変換 (y_test, y_pred_scaled_test ともに (samples, predict_step) の形状を想定)
            y_pred_original_test = self._inverse_transform_predictions(scaler_sp500, y_pred_scaled_test, len(scaled_cols_sp500))
            y_test_original_test = self._inverse_transform_predictions(scaler_sp500, y_test, len(scaled_cols_sp500))

            epsilon = 1e-8 # MAPE計算時のゼロ除算防止
            mape_test = np.mean(np.abs((y_test_original_test - y_pred_original_test) / (y_test_original_test + epsilon))) * 100
            self.logger.info(f"'{model_name}'モデル ({target_ticker}) 学習完了。テストMAPE: {mape_test:.2f}%")

            # 最新データでの予測
            latest_input_sequence_scaled = scaled_data_sp500[-time_step:]
            latest_prediction_original = np.full(predict_step, np.nan) # デフォルトはNaN
            if len(latest_input_sequence_scaled) == time_step:
                latest_pred_scaled = model.predict(np.expand_dims(latest_input_sequence_scaled, axis=0))
                latest_prediction_original = self._inverse_transform_predictions(scaler_sp500, latest_pred_scaled, len(scaled_cols_sp500)).flatten()
            else:
                self.logger.warning(f"'{model_name}' 最新予測用データ不足 ({len(latest_input_sequence_scaled)}/{time_step})。予測スキップ。")

            # テストデータのインデックス特定 (プロット用)
            # X_testの元になったデータのインデックス範囲 (original_indices_sp500 を使用)
            # X_allは scaled_data から作られている。X_testはX_allの後半。
            # y_testの最初の要素は、scaled_data[train_size + time_step] の predict_step 後に対応
            test_start_original_idx_pos = len(X_train) # X_trainのサンプル数
            # y_testに対応する元データのインデックス
            # y_testの各要素は predict_step 日間の予測なので、最初の日のインデックスを代表とする
            test_indices_for_y = original_indices_sp500[test_start_original_idx_pos + time_step : test_start_original_idx_pos + time_step + len(y_test)]


            trained_models_output[model_name] = {
                "model": model, # 保存はパスで行い、ここではオブジェクトを直接持たない方がメモリ効率良い場合も
                "model_name_used": model_name, # for saving
                "y_pred_original_test": y_pred_original_test,
                "y_test_original_test": y_test_original_test, # y_testのプロット用インデックスも必要
                "test_data_indices_for_plot": test_indices_for_y,
                "mape_test": mape_test,
                "latest_prediction_original": latest_prediction_original,
                "last_actual_data_date_for_latest_pred": original_indices_sp500[-1], # 最新予測の基準日
                "predict_step": predict_step,
                "time_step_used": time_step,
                "training_params": {
                    "lstm_units": final_lstm_units, "n_lstm_layers": final_n_layers,
                    "dropout_rate": final_dropout, "learning_rate": (
    model.optimizer.learning_rate.numpy().item()
    if hasattr(model.optimizer, "learning_rate") and hasattr(model.optimizer.learning_rate, "numpy")
    else model.optimizer.learning_rate if hasattr(model.optimizer, "learning_rate")
    else 'N/A'
),
                    "batch_size": final_batch_size, "epochs_trained": len(history.history['loss']),
                    "duration_ms": round(training_duration_model_ms,2)
                }
            }

            model_save_path_template = self.model_settings.get("model_save_path_template", "models/model_{ticker}_{name}.keras")
            model_save_path = model_save_path_template.format(ticker=target_ticker.replace("^",""), name=model_name)
            try:
                save_dir = os.path.dirname(model_save_path)
                if save_dir and not os.path.exists(save_dir): os.makedirs(save_dir)
                save_model(model, model_save_path, save_format="keras") # .keras形式で保存
                self.logger.info(f"'{model_name}'モデル ({target_ticker}) を '{model_save_path}' に保存。")
            except Exception as e:
                self.logger.error(f"モデル保存エラー ({model_save_path}): {e}", exc_info=True)

            # パフォーマンスログ
            perf_log_entry = trained_models_output[model_name]["training_params"].copy()
            perf_log_entry["mape_test"] = mape_test
            self.logger_manager.log_performance(f"train_model_{target_ticker}_{model_name}", perf_log_entry)
            gc.collect()

        self.logger.info(f"S&P500 LSTMモデル群の学習処理完了。総所要時間: {(datetime.now() - start_time_train_all).total_seconds():.2f}秒")
        return trained_models_output


class MarketVisualizer:
    """市場データと予測の可視化クラス"""

    def __init__(self, config: 'Config', logger_manager: LoggerManager):
        self.config = config
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.viz_settings = config.get("visualization_settings", {})
        self.plot_days = self.viz_settings.get("plot_recent_days_count", 365)
        self.save_filename_template = self.viz_settings.get("plot_save_filename_template", "market_prediction_{ticker}.png")
        self.download_dir_candidates = self.viz_settings.get("plot_download_directory_candidates", ["Downloads", "ダウンロード", "."])
        self.dpi = self.viz_settings.get("plot_image_dpi", 300)
        self.ma_windows_plot = self.config.get("feature_engineering_settings.ma_windows", [5,20,60,120]) # FE設定から取得

    def _determine_save_path(self, ticker_symbol: str) -> str:
        home_dir = os.path.expanduser("~")
        filename = self.save_filename_template.format(ticker=ticker_symbol.replace("^",""))
        for dir_candidate in self.download_dir_candidates:
            candidate_path = os.path.join(home_dir, dir_candidate)
            if os.path.isdir(candidate_path):
                return os.path.join(candidate_path, filename)
        # 適切な候補ディレクトリが見つからなければカレントワーキングディレクトリに保存
        return os.path.join(os.getcwd(), filename)

    def plot_predictions_for_sp500(
        self, market_data_dict: Dict[str, Dict[str, Any]],
        trained_models_results: Dict[str, Dict[str, Any]]
    ) -> Optional[str]:
        target_ticker = "^GSPC"
        self.logger.info(f"グラフ作成開始 ({target_ticker})...")

        market_entry = market_data_dict.get(target_ticker)
        if not market_entry or "df" not in market_entry or market_entry["df"].empty:
            self.logger.error(f"{target_ticker}: 市場データなし。グラフ作成中止。")
            return None
        df_sp500 = market_entry["df"]
        ticker_name = market_entry.get("name", target_ticker)

        # サブプロット数 (価格+短期予測+テスト予測, 価格+MA, 相関)
        num_subplots = 3
        fig, axes = plt.subplots(num_subplots, 1, figsize=(18, 6 * num_subplots), sharex=False)
        plt.style.use('seaborn-v0_8-darkgrid') # スタイルの適用 (v0.8以降の推奨名)

        plot_successful = False
        try:
            df_plot_recent = df_sp500.tail(self.plot_days).copy()

            # 1. 価格と短期予測、テスト期間の予測もプロット
            short_model_key = "short" # configの `sp500_prediction_model_configs` のキーと合わせる
            self._plot_price_and_predictions(
                axes[0], df_plot_recent, trained_models_results.get(short_model_key),
                target_ticker, ticker_name, model_label_suffix=f"({short_model_key.capitalize()}-Term)"
            )

            # 2. 価格と移動平均線、クロスシグナル
            self._plot_price_and_moving_avg(axes[1], df_plot_recent, target_ticker, ticker_name)

            # 3. 相関ヒートマップ
            corr_cols = self.viz_settings.get("correlation_matrix_features", [])
            self._plot_correlation_heatmap(axes[2], df_plot_recent, target_ticker, ticker_name, corr_cols)

            fig.suptitle(f"{ticker_name} ({target_ticker}) 市場分析と予測 ({datetime.now().strftime('%Y-%m-%d')})", fontsize=20, y=1.01)
            plt.tight_layout(rect=[0, 0.02, 1, 0.99]) # rectでタイトルとの間隔調整
            plot_successful = True

        except Exception as e:
            self.logger.error(f"グラフ描画中にエラー発生 ({target_ticker}): {e}", exc_info=True)
        finally:
            if plot_successful:
                save_path = self._determine_save_path(target_ticker)
                try:
                    plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
                    self.logger.info(f"グラフを'{save_path}'に保存しました。")
                    plt.close(fig) # 保存後閉じる
                    return save_path
                except Exception as e_save:
                    self.logger.error(f"グラフ保存失敗 ({save_path}): {e_save}", exc_info=True)
            if 'fig' in locals(): plt.close(fig) # 何かあれば必ず閉じる
        return None

    def _plot_price_and_predictions(self, ax: Axes, df_plot_base: pd.DataFrame,
                                    model_result: Optional[Dict[str, Any]],
                                    ticker_symbol: str, ticker_name: str, model_label_suffix: str = "") -> None:
        ax.plot(df_plot_base.index, df_plot_base["Close"], label=f"実績値 ({ticker_name})", color='dodgerblue', lw=1.8, alpha=0.8)

        title = f"{ticker_name} 価格"
        if model_result:
            predict_step = model_result.get("predict_step", 0)
            time_step_model = model_result.get("time_step_used", 0)
            mape = model_result.get("mape_test", float('nan'))
            title += f" と LSTM {predict_step}日間予測 {model_label_suffix} (MAPE: {mape:.2f}%)"

            # テスト期間の予測プロット
            y_test_orig = model_result.get("y_test_original_test")
            y_pred_orig_test = model_result.get("y_pred_original_test")
            test_indices = model_result.get("test_data_indices_for_plot")

            if y_test_orig is not None and y_pred_orig_test is not None and test_indices is not None and len(test_indices) == len(y_test_orig):
                # y_test_orig, y_pred_orig_test は (num_samples, predict_step) の形状
                # ここでは最初の予測ステップ (翌日予測に相当) のみをプロットする
                # 全ステッププロットは複雑になるので別途検討
                ax.plot(test_indices, y_pred_orig_test[:, 0], label=f"テスト期間予測 (LSTM {model_label_suffix.strip()})", color='darkorange', linestyle='-.', lw=1.5, alpha=0.9)

            # 最新の予測プロット
            latest_pred = model_result.get("latest_prediction_original")
            if latest_pred is not None and len(latest_pred) > 0:
                last_actual_date = model_result.get("last_actual_data_date_for_latest_pred", df_plot_base.index[-1])
                # 予測期間のインデックス (元のDFのfreqを考慮)
                freq = pd.infer_freq(df_plot_base.index) or 'B' # B: 営業日
                pred_index_future = pd.date_range(start=last_actual_date + pd.Timedelta(days=1), periods=len(latest_pred), freq=freq)

                ax.plot(pred_index_future, latest_pred, label=f"最新予測 (LSTM {model_label_suffix.strip()})", color='tomato', linestyle='--', marker='o', markersize=4, lw=1.8)
                # 実績の最終値と予測の開始値を結ぶ
                ax.plot([last_actual_date, pred_index_future[0]],
                        [df_plot_base.loc[last_actual_date, "Close"] if last_actual_date in df_plot_base.index else df_plot_base["Close"].iloc[-1], latest_pred[0]],
                        linestyle=':', color='dimgray', alpha=0.7)
        else:
            title += " (予測データなし)"

        ax.set_title(title, fontsize=14)
        ax.set_ylabel("価格", fontsize=12)
        ax.legend(fontsize=10, loc='upper left')
        ax.grid(True, linestyle=':', alpha=0.6)
        from matplotlib.dates import DateFormatter, MonthLocator
        ax.xaxis.set_major_formatter(DateFormatter("%Y-%m"))
        ax.xaxis.set_major_locator(MonthLocator(interval=max(1, len(df_plot_base)//150))) # X軸ラベル数を調整
        ax.tick_params(axis='x', rotation=30)

    def _plot_price_and_moving_avg(self, ax: Axes, df_plot_base: pd.DataFrame, ticker_symbol: str, ticker_name: str) -> None:
        ax.plot(df_plot_base.index, df_plot_base["Close"], label=f"実績値 ({ticker_name})", color='dodgerblue', lw=1.8, alpha=0.8)
        ma_colors = ['darkorange', 'forestgreen', 'mediumpurple', 'sienna']

        for i, window in enumerate(self.ma_windows_plot):
            ma_col = f"MA{window}"
            if ma_col in df_plot_base.columns:
                ax.plot(df_plot_base.index, df_plot_base[ma_col], label=f"MA{window}", color=ma_colors[i % len(ma_colors)], lw=1.2, alpha=0.9)

        # クロスシグナルのプロット
        short_ma_col = f"MA{self.ma_windows_plot[0]}" if self.ma_windows_plot else None
        if "golden_cross" in df_plot_base.columns and short_ma_col and short_ma_col in df_plot_base.columns:
            gc_points = df_plot_base[df_plot_base["golden_cross"]]
            if not gc_points.empty:
                ax.scatter(gc_points.index, gc_points[short_ma_col], label="Golden Cross", marker='^', color='gold', s=120, edgecolor='black', zorder=10)
        if "death_cross" in df_plot_base.columns and short_ma_col and short_ma_col in df_plot_base.columns:
            dc_points = df_plot_base[df_plot_base["death_cross"]]
            if not dc_points.empty:
                ax.scatter(dc_points.index, dc_points[short_ma_col], label="Death Cross", marker='v', color='crimson', s=120, edgecolor='black', zorder=10)

        ax.set_title(f"{ticker_name} 価格と移動平均線", fontsize=14)
        ax.set_ylabel("価格", fontsize=12)
        ax.legend(fontsize=10, loc='upper left')
        ax.grid(True, linestyle=':', alpha=0.6)
        from matplotlib.dates import DateFormatter, MonthLocator
        ax.xaxis.set_major_formatter(DateFormatter("%Y-%m"))
        ax.xaxis.set_major_locator(MonthLocator(interval=max(1, len(df_plot_base)//150)))
        ax.tick_params(axis='x', rotation=30)

    def _plot_correlation_heatmap(self, ax: Axes, df_plot_base: pd.DataFrame, ticker_symbol: str, ticker_name: str, corr_columns: List[str]) -> None:
        available_cols = [col for col in corr_columns if col in df_plot_base.columns and df_plot_base[col].nunique(dropna=True) > 1]
        if len(available_cols) < 2:
            ax.text(0.5, 0.5, "相関分析に十分な列がありません", ha='center', va='center', fontsize=12, transform=ax.transAxes)
            ax.set_title(f'{ticker_name} 相関ヒートマップ (データ不足)', fontsize=14)
            ax.axis('off')
            return

        corr_matrix = df_plot_base[available_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm_r', fmt=".2f", vmin=-1, vmax=1,
                    linewidths=.5, cbar=True, ax=ax, annot_kws={"size": 9}, square=True)
        ax.set_title(f'{ticker_name} 主要指標の相関 (過去{self.plot_days}日間)', fontsize=14)
        
        # 修正: tick_params から ha パラメータを削除し、別途設定
        ax.tick_params(axis='x', rotation=45, labelsize=10)
        ax.tick_params(axis='y', rotation=0, labelsize=10)
        
        # X軸ラベルの水平配置を個別に設定
        for label in ax.get_xticklabels():
            label.set_horizontalalignment('right')
class AdvisorConfigLoader:
    """投資アドバイザー用設定ファイルの読み込み・プロファイル管理クラス"""
    DEFAULT_ADVISOR_CONFIG = {
        "report_filename_template": "market_analysis_report_{profile}.json", # プロファイルごとにも可能
        "profiles": {
            "natural": {
                "profile_description": "標準的なバランス型。基本閾値でシグナル判定。",
                "signal_thresholds": {"buy": 2, "sell": 2}, "vix_threshold": 25,
                "short_trend_threshold_pct": 0.5, "error_accept_threshold_pct": 8.0
            },
            "aggressive": {
                "profile_description": "積極型。買い閾値低め、VIX許容高め。",
                "signal_thresholds": {"buy": 1, "sell": 3}, "vix_threshold": 30,
                "short_trend_threshold_pct": 0.2, "error_accept_threshold_pct": 10.0
            },
            "passive": {
                "profile_description": "慎重型。買い閾値高め、VIX許容低め。",
                "signal_thresholds": {"buy": 3, "sell": 1}, "vix_threshold": 20,
                "short_trend_threshold_pct": 1.0, "error_accept_threshold_pct": 6.0
            },
        },
        "technical_analysis_settings": { # 旧 technical_analysis
            "buy_signal_columns": ["golden_cross", "RSI_buy_signal", "MACD_buy_signal", "BB_buy_signal"],
            "sell_signal_columns": ["death_cross", "RSI_sell_signal", "MACD_sell_signal", "BB_sell_signal"],
            "recent_days_for_signal_count": 5,
            "ma_cross_signal_recency_days": 10
        },
    }

    def __init__(self, config_path: str = "advisor_config.json", logger_manager: Optional[LoggerManager] = None):
        self.logger = (logger_manager or APP_LOGGER_MANAGER).get_logger(self.__class__.__name__)
        self.config_path = config_path
        # Configクラスの _deep_update と _load_config を借用 (または共通化)
        temp_main_config_loader = Config() # ダミーインスタンスでメソッド利用
        self.config_data = temp_main_config_loader._load_config(config_path) # デフォルトは渡さない
        # Advisorのデフォルトを適用
        self.config_data = temp_main_config_loader._deep_update(self.DEFAULT_ADVISOR_CONFIG.copy(), self.config_data)

        self.current_profile_name = "natural" # デフォルト
        self.set_profile(self.current_profile_name)
        self.logger.info(f"アドバイザー設定を '{config_path}' からロード。現在のプロファイル: {self.current_profile_name}")


    def set_profile(self, profile_name: str) -> bool:
        profiles = self.config_data.get("profiles", {})
        if profile_name in profiles:
            self.current_profile_name = profile_name
            self.logger.info(f"投資アドバイザープロファイルを '{profile_name}' に変更しました。")
            return True
        else:
            self.logger.warning(f"プロファイル '{profile_name}' は設定に存在しません。'{self.current_profile_name}' を維持します。")
            return False

    def get_profile_list(self) -> List[str]: return list(self.config_data.get("profiles", {}).keys())
    def get_current_profile_config(self) -> Dict[str, Any]: return self.config_data.get("profiles", {}).get(self.current_profile_name, {})
    def get_profile_description(self) -> str: return self.get_current_profile_config().get("profile_description", "説明なし")

    def get_config_value(self, key_path: str, default: Optional[Any] = None) -> Any:
        keys = key_path.split('.')
        # 1. プロファイル固有設定
        val = self.get_current_profile_config()
        for key in keys:
            if isinstance(val, dict) and key in val: val = val[key]
            else: val = None; break # 見つからなければNoneにして共通設定へ
        if val is not None: return val
        # 2. 共通設定
        val_common = self.config_data
        for key in keys:
            if isinstance(val_common, dict) and key in val_common: val_common = val_common[key]
            else: return default # 共通にもなければデフォルト
        return val_common


class MarketDataAnalyzer:
    """市場データ分析のためのユーティリティ関数群"""
    
    def __init__(self, logger_manager: Optional[LoggerManager] = None):
        self.logger = (logger_manager or APP_LOGGER_MANAGER).get_logger(self.__class__.__name__)
    
    @staticmethod
    def get_nested_value(data: dict, keys: list, default=None):
        """ネストされた辞書から値を安全に取得"""
        for key in keys:
            if isinstance(data, dict) and key in data:
                data = data[key]
            else:
                return default
        return data
    
    def find_last_signal_date(self, df: pd.DataFrame, signal_column_name: str) -> Optional[pd.Timestamp]:
        """指定されたシグナル列の最後の発生日を検索"""
        if signal_column_name not in df.columns or df[signal_column_name].dtype != 'bool':
            self.logger.debug(f"シグナル列 '{signal_column_name}' 不在または非bool型。")
            return None
        try:
            true_signals = df.loc[df[signal_column_name]]  # .loc で FutureWarning 回避
            return pd.Timestamp(true_signals.index.max()) if not true_signals.empty else None
        except Exception as e:
            self.logger.warning(f"'{signal_column_name}' 最終シグナル日検索エラー: {e}", exc_info=True)
            return None
    
    def is_date_within_recent_days(self, latest_market_date: pd.Timestamp,
                                   target_event_date: Optional[pd.Timestamp], recent_days_threshold: int) -> bool:
        """指定された日付が最近の閾値日数以内かを判定"""
        if target_event_date is None:
            return False
        if not (isinstance(latest_market_date, pd.Timestamp) and isinstance(target_event_date, pd.Timestamp)):
            self.logger.warning("is_date_within_recent_days: 日付がTimestamp型ではありません。")
            return False
        return (latest_market_date - target_event_date).days <= recent_days_threshold
    
    def calculate_trend_percentage(self, prediction_array: Any, period_name: str = "期間", 
                                  current_market_price: Optional[float] = None) -> float:
        """
        予測配列から現在価格を基準としたトレンド%を計算
        
        Args:
            prediction_array: 予測価格の配列
            period_name: ログ用の期間名
            current_market_price: 現在の市場価格（基準価格）
        
        Returns:
            float: トレンド% (正=上昇、負=下降)
        """
        try:
            # データ型の統一
            if isinstance(prediction_array, pd.Series):
                values = prediction_array.dropna().values
            elif isinstance(prediction_array, np.ndarray):
                values = prediction_array.flatten()
            elif isinstance(prediction_array, list):
                values = np.array([v for v in prediction_array if v is not None and not np.isnan(v)])
            else:
                self.logger.warning(f"{period_name}トレンド計算: 未対応型 {type(prediction_array)}")
                return 0.0
            
            if len(values) < 1:
                self.logger.debug(f"{period_name}トレンド計算: データ点不足 ({len(values)})")
                return 0.0
            
            # 基準価格の決定（現在価格 > 予測の最初の値）
            if current_market_price is not None and current_market_price > 0:
                base_price = current_market_price
            else:
                base_price = values[0]
            
            # 終了価格（予測の最後の値）
            end_price = values[-1]
            
            # バリデーション
            if base_price <= 0 or np.isnan(base_price) or np.isnan(end_price):
                self.logger.debug(f"{period_name}トレンド計算: 無効な価格データ (base: {base_price}, end: {end_price})")
                return 0.0
            
            # トレンド%計算（正=上昇、負=下降）
            trend_pct = ((end_price - base_price) / base_price) * 100
            
            self.logger.debug(f"{period_name}トレンド計算: {base_price:.2f} → {end_price:.2f} = {trend_pct:.2f}%")
            
            return float(trend_pct)
            
        except Exception as e:
            self.logger.warning(f"{period_name}トレンド計算エラー: {e}", exc_info=True)
            return 0.0
    
    def get_sp500_dataframe(self, market_data_dict: Dict[str, Dict[str, Any]]) -> Optional[pd.DataFrame]:
        """市場データ辞書からS&P500 DataFrameを取得"""
        sp500_entry = market_data_dict.get("^GSPC")
        if not sp500_entry or "df" not in sp500_entry or sp500_entry["df"].empty:
            self.logger.error("S&P500 DataFrameが見つからないか空です。")
            return None
        return sp500_entry["df"]

class ReportGenerator:
    """レポートの生成（JSON保存、コンソール出力）"""
    def __init__(self, logger_manager: Optional[LoggerManager] = None):
        self.logger = (logger_manager or APP_LOGGER_MANAGER).get_logger(self.__class__.__name__)

    def save_report_to_json(self, report_data: Dict[str, Any], filename: str):
        self.logger.info(f"分析レポートを '{filename}' に保存試行...")
        try:
            save_dir = os.path.dirname(filename)
            if save_dir and not os.path.exists(save_dir): os.makedirs(save_dir)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str) # default=strでdatetime等に対応
            self.logger.info(f"分析レポートを '{filename}' に保存しました。")
        except IOError as e: self.logger.error(f"レポート '{filename}' 保存IOエラー: {e}")
        except Exception as e: self.logger.error(f"レポート '{filename}' 保存中予期せぬエラー: {e}", exc_info=True)


    def print_basic_report_to_console(self, report_data: Dict[str, Any]):
        if not report_data: self.logger.warning("表示するレポートデータが空です。"); return
        try:
            print("\n" + "="*10 + " 📈 S&P500 積立タイミング分析レポート 📉 " + "="*10)
            print(f"分析日時: {report_data.get('analysis_datetime', 'N/A')}")
            print(f"投資プロファイル: {report_data.get('profile_name', '未設定')} ({report_data.get('profile_description', 'N/A')})")
            print("-" * 60)

            status = report_data.get('market_status', {})
            print(f"■ S&P500 現状:")
            print(f"  - 最新価格 ({status.get('last_price_date', 'N/A')}): {status.get('current_price', 0.0):.2f}")
            if "VIX" in status: print(f"  - VIX指数: {status['VIX']:.2f}")

            preds = report_data.get('ai_predictions', {})
            errors = report_data.get('ai_error_rates', {})
            print("\n■ AI価格予測 (LSTM):")
            if "nextday_price" in preds:
                print(f"  - 翌日予測: {preds['nextday_price']:.2f} (MAPE: {errors.get('nextday_mape', 0.0):.2f}%)")
            short_p = preds.get('short_term', {})
            if "end_price" in short_p:
                print(f"  - 短期({short_p.get('days',0)}日後): {short_p['end_price']:.2f} (トレンド: {short_p.get('trend_pct', 0.0):.2f}%, MAPE: {errors.get('short_mape',0.0):.2f}%)")
            long_p = preds.get('long_term', {})
            if "end_price" in long_p:
                print(f"  - 長期({long_p.get('days',0)}日後): {long_p['end_price']:.2f} (トレンド: {long_p.get('trend_pct', 0.0):.2f}%, MAPE: {errors.get('long_mape',0.0):.2f}%)")

            tech = report_data.get('technical_signals', {})
            print("\n■ テクニカル分析サマリー:")
            print(f"  - MAクロス: {tech.get('ma_cross_status', '情報なし')}")
            recent_days = tech.get('recent_days_for_count',0)
            print(f"  - 直近{recent_days}日のシグナル:")
            buy_c = tech.get('buy_signal_counts', {})
            sell_c = tech.get('sell_signal_counts', {})
            buy_str = ', '.join([f'{k.replace("_signal","")}:{v}' for k,v in buy_c.items() if v>0]) or "なし"
            sell_str = ', '.join([f'{k.replace("_signal","")}:{v}' for k,v in sell_c.items() if v>0]) or "なし"
            print(f"    買いシグナル合計: {tech.get('total_buy_score',0)} ({buy_str})")
            print(f"    売りシグナル合計: {tech.get('total_sell_score',0)} ({sell_str})")

            print("-" * 60)
            print(f"■ 総合積立アドバイス:\n  {report_data.get('overall_advice', '判断材料不足')}")
            print("-" * 60 + "\n")
        except Exception as e: self.logger.error(f"レポートコンソール表示エラー: {e}", exc_info=True)

import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List
from datetime import datetime
from dataclasses import dataclass
import json

@dataclass
class MarketAssessment:
    """市場評価の総合結果"""
    trend: str
    confidence: float
    risk_level: str
    tech_score: float
    ai_reliability: float

class InvestmentAdvisor:
    """AI予測修正版投資アドバイザー"""
    
    PROFILES = {
        "natural": {"buy_threshold": 3, "vix_threshold": 25, "ai_weight": 2.0},
        "aggressive": {"buy_threshold": 2, "vix_threshold": 30, "ai_weight": 1.5},
        "conservative": {"buy_threshold": 5, "vix_threshold": 20, "ai_weight": 3.0}
    }

    def __init__(self, market_data_dict: Dict, trained_models_results: Dict, 
                 logger_manager, advisor_config_file: str = "advisor_config.json", 
                 initial_profile_name: str = "natural"):
        self.logger = logger_manager.get_logger(self.__class__.__name__)
        self.market_data_dict = market_data_dict
        self.trained_models_results = trained_models_results
        self.current_profile = initial_profile_name
        self.profile = self.PROFILES.get(initial_profile_name, self.PROFILES["natural"])
        self.sp500_df = self._get_sp500_data()
        self.calculation_errors = []
        
        # デバッグ情報出力
        self.logger.info(f"利用可能な市場データキー: {list(self.market_data_dict.keys())}")
        self.logger.info(f"利用可能なモデル結果キー: {list(self.trained_models_results.keys())}")
        
        # AI予測データの詳細デバッグ
        self._debug_ai_predictions_detailed()
        
        self.builtin_config = {
            "conservative": {"buy_threshold": 5, "vix_threshold": 20, "ai_weight": 3.0, "confidence_threshold": 0.8},
            "natural": {"buy_threshold": 3, "vix_threshold": 25, "ai_weight": 2.0, "confidence_threshold": 0.6},
            "aggressive": {"buy_threshold": 2, "vix_threshold": 30, "ai_weight": 1.5, "confidence_threshold": 0.4}
        }
        
        self.logger.info(f"InvestmentAdvisor初期化完了 - プロファイル: {self.current_profile}")

    def _extract_prediction_prices(self):
        """各モデルの予測価格を抽出"""
        prediction_prices = {}
        
        try:
            # nextdayモデルの予測価格
            if 'nextday' in self.trained_models_results:
                nextday_result = self.trained_models_results['nextday']
                if 'latest_prediction_original' in nextday_result:
                    nextday_pred = nextday_result['latest_prediction_original']
                    if isinstance(nextday_pred, (list, np.ndarray)) and len(nextday_pred) > 0:
                        prediction_prices['nextday'] = {
                            'price': round(float(nextday_pred[0]), 2),
                            'period': '翌日'
                        }
                    elif isinstance(nextday_pred, (int, float)):
                        prediction_prices['nextday'] = {
                            'price': round(float(nextday_pred), 2),
                            'period': '翌日'
                        }
            
            # shortモデルの予測価格（最終日）
            if 'short' in self.trained_models_results:
                short_result = self.trained_models_results['short']
                if 'latest_prediction_original' in short_result:
                    short_pred = short_result['latest_prediction_original']
                    if isinstance(short_pred, (list, np.ndarray)) and len(short_pred) > 0:
                        prediction_prices['short'] = {
                            'price': round(float(short_pred[-1]), 2),
                            'period': '20日後'
                        }
            
            # longモデルの予測価格（最終日）
            if 'long' in self.trained_models_results:
                long_result = self.trained_models_results['long']
                if 'latest_prediction_original' in long_result:
                    long_pred = long_result['latest_prediction_original']
                    if isinstance(long_pred, (list, np.ndarray)) and len(long_pred) > 0:
                        prediction_prices['long'] = {
                            'price': round(float(long_pred[-1]), 2),
                            'period': '100日後'
                        }
                        
            self.logger.info(f"予測価格抽出完了: {prediction_prices}")
            return prediction_prices
            
        except Exception as e:
            self.logger.error(f"予測価格抽出エラー: {e}")
            return {}

    def _log_prediction_summary(self):
        """予測価格サマリーをログ出力"""
        try:
            prediction_prices = self._extract_prediction_prices()
            
            self.logger.info("=== AI予測価格サマリー ===")
            for model_name, pred_data in prediction_prices.items():
                self.logger.info(f"{model_name}モデル予測価格: ${pred_data['price']:,.2f} ({pred_data['period']})")
            
            # 現在価格との比較
            if not self.sp500_df.empty:
                current_price = self.sp500_df['Close'].iloc[-1]
                self.logger.info(f"現在価格: ${current_price:,.2f}")
                
                if 'nextday' in prediction_prices:
                    change = prediction_prices['nextday']['price'] - current_price
                    change_pct = (change / current_price) * 100
                    self.logger.info(f"翌日予測変化: ${change:+.2f} ({change_pct:+.2f}%)")
                    
        except Exception as e:
            self.logger.error(f"予測サマリーログエラー: {e}")

    def _display_enhanced_report(self, report_data: Dict[str, Any]):
        """強化されたレポート表示"""
        prediction_prices = self._extract_prediction_prices()
        
        print("\n" + "="*60)
        print("        🎯 AI投資分析レポート（強化版） 🎯")
        print("="*60)
        print(f"📅 生成日時: {report_data.get('timestamp', 'N/A')}")
        print(f"👤 投資プロファイル: {self.current_profile.upper()}")
        
        # AI予測価格セクションを追加
        print("\n🔮 【AI予測価格】")
        if 'nextday' in prediction_prices:
            print(f"翌日予測: ${prediction_prices['nextday']['price']:,.2f}")
        if 'short' in prediction_prices:
            print(f"短期予測（20日後）: ${prediction_prices['short']['price']:,.2f}")
        if 'long' in prediction_prices:
            print(f"長期予測（100日後）: ${prediction_prices['long']['price']:,.2f}")
        
        # 既存のセクション
        assessment = report_data.get("assessment", {})
        advice = report_data.get("investment_advice", {})
        market = report_data.get("market_analysis", {}).get("current_status", {})
        ai_predictions = report_data.get("market_analysis", {}).get("ai_predictions", {})
        
        long_term = ai_predictions.get("long_term", {})
        ai_trend = long_term.get("trend_pct", 0)
        ai_confidence = long_term.get("confidence", 0)
        
        print(f"\n🤖 【AI予測分析】")
        print(f"長期トレンド予測: {ai_trend:+.2f}%")
        print(f"AI信頼度: {ai_confidence:.1%}")
        
        print(f"\n📊 【総合評価】")
        print(f"トレンド: {assessment.get('overall_trend', 'N/A').upper()}")
        print(f"信頼度: {assessment.get('confidence_score', 0):.1%}")
        print(f"リスク: {assessment.get('risk_level', 'N/A').upper()}")
        print(f"テクニカルスコア: {assessment.get('technical_score', 0):.2f}")
        print(f"AI信頼度: {assessment.get('ai_reliability', 0):.1%}")
        
        print(f"\n💡 【推奨アクション】")
        print(f"アクション: {advice.get('primary_action', 'N/A')}")
        print(f"強度: {advice.get('action_strength', 'N/A')}")
        
        profile_advice = advice.get("profile_adjusted_advice", {})
        if profile_advice:
            print(f"アドバイス: {profile_advice.get('advice_text', 'N/A')}")
            print(f"推奨ポジションサイズ: {profile_advice.get('position_sizing', 'N/A')}")
        
        key_factors = advice.get("key_factors", [])
        if key_factors:
            print(f"\n🔍 【主要判断要因】")
            for factor in key_factors:
                print(f"• {factor}")
        
        warnings = advice.get("warnings", [])
        if warnings:
            print(f"\n⚠️ 【警告・注意事項】")
            for i, warning in enumerate(warnings[:3], 1):
                print(f"{i}. {warning}")
        
        if market and "error" not in market:
            print(f"\n📈 【市場状況】")
            print(f"S&P500価格: ${market.get('current_price', 0):.2f}")
            print(f"日次変動: {market.get('daily_change', 0):+.2f}%")
            print(f"VIX指数: {market.get('VIX', 0):.1f} ({market.get('vix_level', 'N/A')})")
            print(f"5日ボラティリティ: {market.get('volatility_5d', 0):.1f}%")
        
        print("="*60)

    def _debug_ai_predictions_detailed(self):
        """AI予測データの詳細デバッグ"""
        self.logger.info("=== AI予測データ詳細分析 ===")
        for model_key, model_result in self.trained_models_results.items():
            self.logger.info(f"\n--- {model_key}モデル詳細 ---")
            if isinstance(model_result, dict):
                for key, value in model_result.items():
                    if isinstance(value, list):
                        if len(value) > 0:
                            self.logger.info(f"  {key}: List[{len(value)}] - 最後の3つ: {value[-3:]}")
                        else:
                            self.logger.info(f"  {key}: 空のリスト")
                    elif isinstance(value, (int, float)):
                        self.logger.info(f"  {key}: {value}")
                    else:
                        self.logger.info(f"  {key}: {type(value)}")
                
                # 特に重要なデータの詳細チェック
                if 'y_pred_original_test' in model_result and 'y_test_original_test' in model_result:
                    pred = model_result['y_pred_original_test']
                    actual = model_result['y_test_original_test']
                    if isinstance(pred, list) and isinstance(actual, list):
                        self.logger.info(f"  予測データ長: {len(pred)}, 実際データ長: {len(actual)}")
                        if len(pred) > 0 and len(actual) > 0:
                            self.logger.info(f"  最後の予測値: {pred[-1]}, 最後の実際値: {actual[-1]}")
                
                if 'latest_prediction_original' in model_result:
                    latest = model_result['latest_prediction_original']
                    self.logger.info(f"  最新予測値: {latest}")

    def _get_sp500_data(self) -> Optional[pd.DataFrame]:
        """S&P500データを安全に取得"""
        try:
            for key in ["^GSPC", "SP500", "SPX", "sp500"]:
                if key in self.market_data_dict:
                    data = self.market_data_dict[key]
                    if isinstance(data, dict) and "df" in data:
                        df = data["df"]
                        if df is not None and not df.empty:
                            self.logger.info(f"S&P500データ読み込み成功: {len(df)}行, 列: {list(df.columns)}")
                            return df
                    elif isinstance(data, pd.DataFrame) and not data.empty:
                        self.logger.info(f"S&P500データ読み込み成功: {len(data)}行, 列: {list(data.columns)}")
                        return data
            
            self.logger.error("S&P500データが見つかりません")
            return pd.DataFrame()
        except Exception as e:
            self.logger.error(f"S&P500データ取得エラー: {e}")
            return pd.DataFrame()

    def _calculate_ai_prediction_from_model_data(self, model_result: Dict, model_type: str) -> tuple[float, float]:
        """モデルデータから実際のAI予測を計算"""
        try:
            # パターン1: latest_prediction_originalがある場合
            if 'latest_prediction_original' in model_result:
                latest_pred = model_result['latest_prediction_original']
                if isinstance(latest_pred, (list, np.ndarray)) and len(latest_pred) > 0:
                    latest_pred = latest_pred[0] if model_type == 'nextday' else latest_pred[-1]
                
                if isinstance(latest_pred, (int, float)) and latest_pred != 0:
                    # 現在の価格を取得
                    current_price = self.sp500_df['Close'].iloc[-1] if not self.sp500_df.empty else latest_pred
                    
                    # 変化率計算
                    change_pct = ((latest_pred - current_price) / current_price) * 100
                    
                    # 信頼度計算（MAPEから）
                    mape = model_result.get('mape_test', 50)
                    confidence = max(0.1, min(0.9, (100 - mape) / 100))
                    
                    self.logger.info(f"{model_type}: 最新予測={latest_pred:.2f}, 現在価格={current_price:.2f}, 変化率={change_pct:.2f}%, MAPE={mape:.2f}%")
                    return change_pct, confidence
            
            # パターン2: y_pred_original_testとy_test_original_testから計算
            if 'y_pred_original_test' in model_result and 'y_test_original_test' in model_result:
                pred_data = model_result['y_pred_original_test']
                actual_data = model_result['y_test_original_test']
                
                if isinstance(pred_data, list) and isinstance(actual_data, list) and len(pred_data) > 0 and len(actual_data) > 0:
                    # 最新の予測と実際の値
                    latest_pred = pred_data[-1]
                    latest_actual = actual_data[-1] if len(actual_data) > 0 else latest_pred
                    
                    # 変化率計算
                    if latest_actual != 0:
                        change_pct = ((latest_pred - latest_actual) / latest_actual) * 100
                    else:
                        change_pct = 0
                    
                    # 信頼度計算（MAPEから）
                    mape = model_result.get('mape_test', 50)
                    confidence = max(0.1, min(0.9, (100 - mape) / 100))
                    
                    self.logger.info(f"{model_type}: 予測={latest_pred:.2f}, 実際={latest_actual:.2f}, 変化率={change_pct:.2f}%, MAPE={mape:.2f}%")
                    return change_pct, confidence
            
            # パターン3: predict_stepを使用した将来予測
            if 'predict_step' in model_result:
                predict_step = model_result['predict_step']
                current_price = self.sp500_df['Close'].iloc[-1] if not self.sp500_df.empty else 5800
                
                # 簡易的な予測（実際のモデルロジックに基づいて調整が必要）
                # ここでは過去のトレンドベースの予測を行う
                if not self.sp500_df.empty and len(self.sp500_df) >= predict_step:
                    past_returns = self.sp500_df['Close'].pct_change().dropna().tail(predict_step)
                    avg_return = past_returns.mean()
                    predicted_price = current_price * (1 + avg_return * predict_step)
                    change_pct = ((predicted_price - current_price) / current_price) * 100
                    
                    mape = model_result.get('mape_test', 30)
                    confidence = max(0.1, min(0.9, (100 - mape) / 100))
                    
                    self.logger.info(f"{model_type}: 段階予測={predict_step}, 変化率={change_pct:.2f}%, MAPE={mape:.2f}%")
                    return change_pct, confidence
            
            self.logger.warning(f"{model_type}: 予測データの計算に失敗")
            return 0.0, 0.5
            
        except Exception as e:
            self.logger.error(f"{model_type} AI予測計算エラー: {e}")
            return 0.0, 0.5

    def _get_ai_predictions_summary(self) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """AI予測のサマリーを取得（修正版）"""
        predictions = {}
        errors = {}
        
        try:
            self.logger.info("=== AI予測サマリー生成開始（修正版） ===")
            
            # 各モデルの結果を個別に処理
            for model_key, model_result in self.trained_models_results.items():
                if not isinstance(model_result, dict):
                    continue
                
                self.logger.info(f"処理中のモデル: {model_key}")
                
                # AI予測を計算
                trend_pct, confidence = self._calculate_ai_prediction_from_model_data(model_result, model_key)
                
                # MAPE取得
                mape = model_result.get('mape_test', 50)
                errors[f"{model_key}_mape"] = mape
                
                # モデルタイプ別に分類
                if 'long' in model_key.lower():
                    predictions["long_term"] = {"trend_pct": trend_pct, "confidence": confidence}
                elif 'short' in model_key.lower():
                    predictions["short_term"] = {"trend_pct": trend_pct, "confidence": confidence}
                elif 'nextday' in model_key.lower() or 'next' in model_key.lower():
                    predictions["nextday_price"] = {"change_pct": trend_pct, "confidence": confidence}
                else:
                    # デフォルトは長期として扱う
                    predictions["long_term"] = {"trend_pct": trend_pct, "confidence": confidence}
            
            # 予測が空の場合の処理
            if not predictions.get("long_term"):
                # 最も信頼できるモデルから長期予測を生成
                best_model = None
                best_mape = float('inf')
                
                for model_key, model_result in self.trained_models_results.items():
                    if isinstance(model_result, dict) and 'mape_test' in model_result:
                        mape = model_result.get('mape_test', 100)
                        if mape < best_mape:
                            best_mape = mape
                            best_model = model_key
                
                if best_model:
                    trend_pct, confidence = self._calculate_ai_prediction_from_model_data(
                        self.trained_models_results[best_model], best_model
                    )
                    predictions["long_term"] = {"trend_pct": trend_pct, "confidence": confidence}
                    self.logger.info(f"最良モデル {best_model} から長期予測生成: {trend_pct:.2f}%")
            
            self.logger.info(f"最終的なAI予測: {predictions}")
            return predictions, errors
            
        except Exception as e:
            self.logger.error(f"AI予測サマリー取得エラー: {e}")
            return {}, {}

    def _get_vix_value(self) -> float:
        """VIX値を取得（S&P500データから直接取得を優先）"""
        try:
            # まずS&P500データ内のVIX列をチェック
            if not self.sp500_df.empty and 'VIX' in self.sp500_df.columns:
                vix_series = self.sp500_df['VIX'].dropna()
                if len(vix_series) > 0:
                    vix_value = float(vix_series.iloc[-1])
                    self.logger.info(f"VIX値取得成功: {vix_value} (S&P500データから)")
                    return vix_value
            
            # 次に市場データ辞書から取得
            self.logger.info("=== VIX値取得開始 ===")
            self.logger.info(f"利用可能なキー: {list(self.market_data_dict.keys())}")
            
            for key in ["VIX", "^VIX", "vix", "volatility"]:
                if key in self.market_data_dict:
                    vix_data = self.market_data_dict[key]
                    self.logger.info(f"VIXキー '{key}' 発見: {type(vix_data)}")
                    
                    if isinstance(vix_data, dict):
                        if "df" in vix_data and not vix_data["df"].empty:
                            vix_df = vix_data["df"]
                            if "Close" in vix_df.columns:
                                vix_value = float(vix_df["Close"].iloc[-1])
                                self.logger.info(f"VIX値取得成功: {vix_value} (from df)")
                                return vix_value
                        elif "Close" in vix_data:
                            close_data = vix_data["Close"]
                            if isinstance(close_data, list) and len(close_data) > 0:
                                vix_value = float(close_data[-1])
                                self.logger.info(f"VIX値取得成功: {vix_value} (from list)")
                                return vix_value
                    elif isinstance(vix_data, (int, float)):
                        self.logger.info(f"VIX値取得成功: {vix_data} (direct)")
                        return float(vix_data)
            
            # S&P500データからボラティリティを計算
            if not self.sp500_df.empty:
                returns = self.sp500_df['Close'].pct_change().dropna().tail(20)
                volatility = returns.std() * np.sqrt(252) * 100
                estimated_vix = min(80, max(10, volatility))
                self.logger.info(f"VIX推定値: {estimated_vix:.1f} (ボラティリティから計算)")
                return estimated_vix
            
            self.logger.warning("VIX値が見つからないため、デフォルト値20.0を使用")
            return 20.0
            
        except Exception as e:
            self.logger.error(f"VIX取得エラー: {e}")
            return 20.0

    def _calculate_technical_indicators(self) -> Dict[str, Any]:
        """テクニカル指標を計算（S&P500データから直接取得を優先）"""
        if self.sp500_df.empty:
            return {}
        
        try:
            tech_data = {}
            
            # RSI（既に計算済みの場合は使用、そうでなければ計算）
            if 'RSI' in self.sp500_df.columns:
                current_rsi = self.sp500_df['RSI'].iloc[-1]
                if pd.notna(current_rsi):
                    tech_data["rsi_current"] = float(current_rsi)
                    if current_rsi >= 70:
                        tech_data["rsi_signal"] = "過買い"
                    elif current_rsi <= 30:
                        tech_data["rsi_signal"] = "過売り"
                    else:
                        tech_data["rsi_signal"] = "中立"
            else:
                # RSI計算
                if len(self.sp500_df) >= 14:
                    delta = self.sp500_df['Close'].diff()
                    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                    rs = gain / loss
                    rsi = 100 - (100 / (1 + rs))
                    
                    current_rsi = rsi.iloc[-1]
                    if pd.notna(current_rsi):
                        tech_data["rsi_current"] = float(current_rsi)
                        if current_rsi >= 70:
                            tech_data["rsi_signal"] = "過買い"
                        elif current_rsi <= 30:
                            tech_data["rsi_signal"] = "過売り"
                        else:
                            tech_data["rsi_signal"] = "中立"
            
            # 移動平均線（既に計算済みの場合は使用）
            df = self.sp500_df.copy()
            current_price = df['Close'].iloc[-1]
            
            # 既存のMA列があるかチェック
            ma_columns = [col for col in df.columns if col.startswith('MA') and col[2:].isdigit()]
            if ma_columns:
                # 既存のMA列を使用
                ma_signals = {}
                for ma_col in ['MA5', 'MA20', 'MA50', 'MA60', 'MA120']:
                    if ma_col in df.columns:
                        ma_value = df[ma_col].iloc[-1]
                        if pd.notna(ma_value):
                            ma_signals[f"price_vs_{ma_col.lower()}"] = "above" if current_price > ma_value else "below"
                
                # MA同士の比較
                if 'MA5' in df.columns and 'MA20' in df.columns:
                    ma5_val = df['MA5'].iloc[-1]
                    ma20_val = df['MA20'].iloc[-1]
                    if pd.notna(ma5_val) and pd.notna(ma20_val):
                        ma_signals["ma5_vs_ma20"] = "above" if ma5_val > ma20_val else "below"
                
                tech_data["ma_signals"] = ma_signals
            else:
                # MA計算
                if len(df) >= 50:
                    df['MA5'] = df['Close'].rolling(5).mean()
                    df['MA20'] = df['Close'].rolling(20).mean()
                    df['MA50'] = df['Close'].rolling(50).mean()
                    
                    latest = df.iloc[-1]
                    tech_data["ma_signals"] = {
                        "price_vs_ma5": "above" if current_price > latest['MA5'] else "below",
                        "price_vs_ma20": "above" if current_price > latest['MA20'] else "below",
                        "price_vs_ma50": "above" if current_price > latest['MA50'] else "below",
                        "ma5_vs_ma20": "above" if latest['MA5'] > latest['MA20'] else "below"
                    }
            
            # ゴールデンクロス・デッドクロスチェック
            if 'golden_cross' in self.sp500_df.columns and 'death_cross' in self.sp500_df.columns:
                # 最近のクロス信号をチェック
                recent_data = self.sp500_df.tail(30)  # 過去30日
                golden_cross_recent = recent_data['golden_cross'].any()
                death_cross_recent = recent_data['death_cross'].any()
                
                if golden_cross_recent:
                    tech_data["recent_cross"] = "golden"
                elif death_cross_recent:
                    tech_data["recent_cross"] = "death"
                else:
                    tech_data["recent_cross"] = "none"
            
            self.logger.info(f"テクニカル指標計算完了: {tech_data}")
            return tech_data
            
        except Exception as e:
            self.logger.error(f"テクニカル指標計算エラー: {e}")
            return {}

    def _get_technical_signals_summary(self) -> Dict[str, Any]:
        """テクニカルシグナルのサマリー（改良版）"""
        try:
            summary = {
                "ma_cross_status": "MAクロスは30日以内になし",
                "total_buy_score": 0,
                "total_sell_score": 0,
                "recent_days_for_count": 15,
                "rsi_signal": "中立"
            }
            
            # テクニカル指標を計算
            tech_indicators = self._calculate_technical_indicators()
            summary.update(tech_indicators)
            
            # 買い売りスコア計算（改良版）
            buy_score = 0
            sell_score = 0
            
            # RSIベースのスコア
            if "rsi_signal" in tech_indicators:
                if tech_indicators["rsi_signal"] == "過売り":
                    buy_score += 3
                elif tech_indicators["rsi_signal"] == "過買い":
                    sell_score += 3
                else:
                    # 中立でも微細な判定
                    rsi_val = tech_indicators.get("rsi_current", 50)
                    if rsi_val < 40:
                        buy_score += 1
                    elif rsi_val > 60:
                        sell_score += 1
            
            # 移動平均ベースのスコア
            if "ma_signals" in tech_indicators:
                ma_signals = tech_indicators["ma_signals"]
                
                # 価格と移動平均の関係
                above_count = sum(1 for key, value in ma_signals.items() 
                                if key.startswith("price_vs_") and value == "above")
                below_count = sum(1 for key, value in ma_signals.items() 
                                if key.startswith("price_vs_") and value == "below")
                
                if above_count > below_count:
                    buy_score += above_count
                else:
                    sell_score += below_count
                
                # 短期MAが長期MAを上回る場合
                if ma_signals.get("ma5_vs_ma20") == "above":
                    buy_score += 1
                else:
                    sell_score += 1
            
            # クロス信号
            if "recent_cross" in tech_indicators:
                if tech_indicators["recent_cross"] == "golden":
                    buy_score += 2
                    summary["ma_cross_status"] = "直近ゴールデンクロス発生"
                elif tech_indicators["recent_cross"] == "death":
                    sell_score += 2
                    summary["ma_cross_status"] = "直近デッドクロス発生"
            
            summary["total_buy_score"] = buy_score
            summary["total_sell_score"] = sell_score
            
            self.logger.info(f"テクニカルサマリー: 買い={buy_score}, 売り={sell_score}")
            return summary
            
        except Exception as e:
            self.logger.error(f"テクニカル分析エラー: {e}")
            return {"error": str(e)}

    def _generate_comprehensive_market_assessment(self, market_status: Dict, predictions: Dict, 
                                                errors: Dict, tech_signals: Dict) -> MarketAssessment:
        """総合市場評価を生成（AI予測重視版）"""
        try:
            # AI予測分析（重要度を高める）
            long_term_trend = predictions.get("long_term", {}).get("trend_pct", 0)
            ai_confidence = predictions.get("long_term", {}).get("confidence", 0.5)
            
            self.logger.info(f"AI分析: 長期トレンド={long_term_trend:.2f}%, 信頼度={ai_confidence:.2f}")
            
            # AI予測の有効性チェック
            has_meaningful_ai_prediction = abs(long_term_trend) > 0.5 and ai_confidence > 0.3
            
            # 基本トレンド判定（AI予測を重視）
            if has_meaningful_ai_prediction:
                if long_term_trend < -3 and ai_confidence > 0.5:
                    trend = "bearish"
                    confidence = min(0.9, ai_confidence + 0.2)
                elif long_term_trend > 2 and ai_confidence > 0.5:
                    trend = "bullish"
                    confidence = min(0.9, ai_confidence + 0.2)
                elif abs(long_term_trend) > 1:
                    trend = "bullish" if long_term_trend > 0 else "bearish"
                    confidence = ai_confidence
                else:
                    trend = "neutral"
                    confidence = ai_confidence * 0.8
            else:
                # AI予測が不十分な場合はテクニカル分析を重視
                buy_score = tech_signals.get("total_buy_score", 0)
                sell_score = tech_signals.get("total_sell_score", 0)
                
                if buy_score > sell_score + 2:
                    trend = "bullish"
                    confidence = 0.6
                elif sell_score > buy_score + 2:
                    trend = "bearish"
                    confidence = 0.6
                else:
                    trend = "neutral"
                    confidence = 0.4
                
                self.logger.info(f"AI予測不十分のためテクニカル重視: 買い={buy_score}, 売り={sell_score}")
            
            # テクニカル分析との統合
            buy_score = tech_signals.get("total_buy_score", 0)
            sell_score = tech_signals.get("total_sell_score", 0)
            
            # AI予測とテクニカルが矛盾する場合の調整
            if has_meaningful_ai_prediction:
                if trend == "bullish" and sell_score > buy_score + 2:
                    trend = "neutral"
                    confidence *= 0.8
                    self.logger.info("AI楽観予測だがテクニカル悲観のため中立に調整")
                elif trend == "bearish" and buy_score > sell_score + 2:
                    trend = "neutral"
                    confidence *= 0.8
                    self.logger.info("AI悲観予測だがテクニカル楽観のため中立に調整")
                elif trend == "neutral":
                    if buy_score > sell_score + 1:
                        trend = "bullish"
                        confidence += 0.1
                    elif sell_score > buy_score + 1:
                        trend = "bearish"
                        confidence += 0.1
            
            # リスクレベル判定（改良版）
            vix_value = market_status.get("VIX", 20)
            volatility = market_status.get("volatility_5d", 0)
            
            risk_factors = 0
            if vix_value > 30:
                risk_factors += 3
            elif vix_value > 25:
                risk_factors += 2
            elif vix_value > 20:
                risk_factors += 1
            
            if volatility > 25:
                risk_factors += 2
            elif volatility > 15:
                risk_factors += 1
            
            # AI予測の不確実性
            if not has_meaningful_ai_prediction:
                risk_factors += 2
            elif ai_confidence < 0.4:
                risk_factors += 1
            
            if risk_factors >= 5:
                risk_level = "high"
            elif risk_factors >= 2:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            # テクニカルスコア計算
            tech_score = 0.5
            total_signals = buy_score + sell_score
            if total_signals > 0:
                tech_score = buy_score / total_signals
            tech_score = max(0.0, min(1.0, tech_score))
            
            # AI信頼度（調整版）
            ai_reliability = ai_confidence if has_meaningful_ai_prediction else 0.3
            
            result = MarketAssessment(
                trend=trend,
                confidence=max(0.1, min(0.9, confidence)),
                risk_level=risk_level,
                tech_score=tech_score,
                ai_reliability=ai_reliability
            )
            
            self.logger.info(f"総合評価: {trend}, 信頼度={result.confidence:.2f}, リスク={risk_level}, AI有効={has_meaningful_ai_prediction}")
            return result
            
        except Exception as e:
            self.logger.error(f"市場評価生成エラー: {e}")
            return MarketAssessment("neutral", 0.3, "high", 0.5, 0.3)

    def _get_current_config(self) -> Dict[str, Any]:
        return self.builtin_config.get(self.current_profile, self.builtin_config["natural"])

    def _get_current_profile_name(self) -> str:
        return self.current_profile

    def _get_current_market_status(self) -> Dict[str, Any]:
        if self.sp500_df is None or self.sp500_df.empty:
            return {"error": "データ不足"}
        
        try:
            latest_row = self.sp500_df.iloc[-1]
            current_price = float(latest_row["Close"])
            
            daily_change = 0
            if len(self.sp500_df) > 1:
                prev_price = float(self.sp500_df["Close"].iloc[-2])
                daily_change = ((current_price - prev_price) / prev_price) * 100
            
            vix_value = self._get_vix_value()
            
            volatility_5d = 0
            if len(self.sp500_df) >= 5:
                returns = self.sp500_df["Close"].pct_change().dropna().tail(5)
                volatility_5d = float(returns.std() * np.sqrt(252) * 100)
            
            status = {
                "current_price": current_price,
                "last_price_date": self.sp500_df.index[-1].strftime("%Y-%m-%d"),
                "volume": float(latest_row.get("Volume", 0)),
                "daily_change": daily_change,
                "volatility_5d": volatility_5d,
                "VIX": vix_value,
                "vix_level": self._categorize_vix(vix_value)
            }
            
            self.logger.info(f"市場状況: 価格=${current_price:.2f}, 変動={daily_change:.2f}%, VIX={vix_value:.1f}")
            return status
            
        except Exception as e:
            self.logger.error(f"市場状況取得エラー: {e}")
            return {"error": str(e)}

    def _categorize_vix(self, vix_value: float) -> str:
        if vix_value < 15:
            return "低位安定"
        elif vix_value < 25:
            return "通常範囲"
        elif vix_value < 35:
            return "警戒レベル"
        else:
            return "パニックレベル"

    def _generate_investment_advice_from_assessment(self, assessment: MarketAssessment) -> Dict[str, Any]:
        try:
            config = self._get_current_config()
            confidence_threshold = config.get("confidence_threshold", 0.6)
            
            predictions, _ = self._get_ai_predictions_summary()
            ai_trend = predictions.get("long_term", {}).get("trend_pct", 0)
            
            # より詳細なアクション決定
            if assessment.confidence < confidence_threshold:
                primary_action = "HOLD"
                action_strength = "弱"
                reason = f"信頼度{assessment.confidence:.1%}が閾値{confidence_threshold:.1%}を下回る"
            elif assessment.trend == "bullish" and assessment.risk_level != "high":
                if abs(ai_trend) > 1 and ai_trend > 0:
                    primary_action = "BUY"
                    action_strength = "強" if assessment.confidence > 0.8 and ai_trend > 3 else "中"
                    reason = f"上昇トレンド確認(AI: +{ai_trend:.1f}%)"
                else:
                    primary_action = "BUY" 
                    action_strength = "弱"
                    reason = "テクニカル上昇だがAI予測不明確"
            elif assessment.trend == "bearish" or (abs(ai_trend) > 1 and ai_trend < -2):
                primary_action = "SELL"
                action_strength = "強" if ai_trend < -3 else "中"
                reason = f"下降トレンド(AI: {ai_trend:.1f}%)"
            else:
                primary_action = "HOLD"
                action_strength = "中"
                reason = "明確なトレンドなし"
            
            # 警告生成（詳細版）
            warnings = []
            if ai_trend < -5:
                warnings.append(f"🚨 AI予測が大幅下落を警告: {ai_trend:.1f}%")
            elif ai_trend < -2:
                warnings.append(f"⚠️ AI予測が下落を示唆: {ai_trend:.1f}%")
            elif ai_trend > 5:
                warnings.append(f"📈 AI予測が大幅上昇を示唆: +{ai_trend:.1f}%")
            
            if assessment.risk_level == "high":
                warnings.append("⚠️ 高リスク市場環境")
            if assessment.confidence < 0.4:
                warnings.append("⚠️ 予測信頼度が低い状況")
            if assessment.ai_reliability < 0.4:
                warnings.append("⚠️ AI予測の信頼性が低い")
            
            warnings.extend([
                "投資は元本保証がありません。余裕資金での投資を心がけてください。",
                "このアドバイスは情報提供目的であり、投資の最終判断はご自身で行ってください。"
            ])
            
            return {
                "primary_action": primary_action,
                "action_strength": action_strength,
                "risk_assessment": f"{assessment.risk_level}リスク",
                "confidence_score": assessment.confidence,
                "profile_adjusted_advice": {
                    "advice_text": f"【{self.current_profile.upper()}】{primary_action}推奨（{action_strength}）- {reason}",
                    "position_sizing": self._get_position_sizing_advice(primary_action, assessment.risk_level)
                },
                "key_factors": [
                    f"AI予測: {ai_trend:+.1f}%",
                    f"市場トレンド: {assessment.trend}",
                    f"信頼度: {assessment.confidence:.1%}",
                    f"リスクレベル: {assessment.risk_level}",
                    f"AI信頼度: {assessment.ai_reliability:.1%}"
                ],
                "recommendations": self._get_basic_recommendations(primary_action),
                "warnings": warnings[:5]
            }
            
        except Exception as e:
            self.logger.error(f"投資アドバイス生成エラー: {e}")
            return {
                "error": str(e),
                "primary_action": "HOLD",
                "action_strength": "弱",
                "risk_assessment": "データ不足により高リスク"
            }

    def _get_position_sizing_advice(self, action: str, risk_level: str) -> str:
        if action == "HOLD":
            return "現状維持"
        
        profile_multipliers = {"conservative": 0.5, "natural": 1.0, "aggressive": 1.5}
        risk_multipliers = {"low": 1.0, "medium": 0.8, "high": 0.5}
        
        base_size = 10
        multiplier = profile_multipliers.get(self.current_profile, 1.0) * risk_multipliers.get(risk_level, 0.8)
        recommended_size = int(base_size * multiplier)
        
        return f"資金の{recommended_size}%程度"

    def _get_basic_recommendations(self, action: str) -> List[str]:
        recommendations = {
            "BUY": [
                "ETFやインデックスファンドでのS&P500への投資を検討",
                "ドルコスト平均法による段階的な投資実行",
                "投資前に緊急資金（生活費3-6ヶ月分）の確保確認"
            ],
            "SELL": [
                "段階的な利確を検討",
                "現金ポジションの増加",
                "税務効率を考慮した売却タイミング調整"
            ],
            "HOLD": [
                "現在のポジション維持",
                "市場動向の継続的な監視",
                "投資機会の準備"
            ]
        }
        return recommendations.get(action, recommendations["HOLD"])

    def generate_investment_advice(self) -> Dict[str, Any]:
        try:
            self.logger.info(f"投資アドバイスレポート生成開始 (プロファイル: {self.current_profile})")
            
            market_status = self._get_current_market_status()
            predictions, errors = self._get_ai_predictions_summary()
            tech_signals = self._get_technical_signals_summary()
            
            assessment = self._generate_comprehensive_market_assessment(
                market_status, predictions, errors, tech_signals
            )
            
            advice = self._generate_investment_advice_from_assessment(assessment)
            
            return {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "profile": self.current_profile,
                    "calculation_errors": self.calculation_errors
                },
                "market_analysis": {
                    "current_status": market_status,
                    "ai_predictions": predictions,
                    "ai_reliability": errors,
                    "technical_signals": tech_signals
                },
                "assessment": {
                    "overall_trend": assessment.trend,
                    "confidence_score": assessment.confidence,
                    "risk_level": assessment.risk_level,
                    "technical_score": assessment.tech_score,
                    "ai_reliability": assessment.ai_reliability
                },
                "investment_advice": advice
            }
            
        except Exception as e:
            self.logger.error(f"投資アドバイス生成エラー: {e}")
            return {"error": str(e)}

    def generate_investment_report(self, save_to_json: bool = False, print_to_console: bool = True) -> bool:
        try:
            self.logger.info("投資レポート生成を開始します")
            report_data = self.generate_investment_advice()
            
            if "error" in report_data:
                self.logger.error(f"レポート生成エラー: {report_data['error']}")
                return False
            
            # 予測価格のログ出力を追加
            self._log_prediction_summary()
            
            # 強化版レポート表示に変更
            if print_to_console:
                # 予測価格データを追加
                report_data['prediction_prices'] = self._extract_prediction_prices()
                report_data['timestamp'] = datetime.now().isoformat()
                self._display_enhanced_report(report_data)
            
            if save_to_json:
                # 予測価格データをJSONに含める
                report_data['prediction_prices'] = self._extract_prediction_prices()
                self._save_report_to_json(report_data)
            
            self.logger.info("投資レポート生成が正常に完了しました")
            return True
        except Exception as e:
            self.logger.error(f"レポート生成エラー: {e}")
            return False

    def _print_report(self, data: Dict[str, Any]):
        print("\n" + "="*60)
        print("        🎯 AI投資分析レポート（修正版） 🎯")
        print("="*60)
        
        metadata = data.get("metadata", {})
        print(f"📅 生成日時: {metadata.get('generated_at', 'N/A')}")
        print(f"👤 投資プロファイル: {metadata.get('profile', 'N/A').upper()}")
        
        assessment = data.get("assessment", {})
        advice = data.get("investment_advice", {})
        market = data.get("market_analysis", {}).get("current_status", {})
        ai_predictions = data.get("market_analysis", {}).get("ai_predictions", {})
        
        long_term = ai_predictions.get("long_term", {})
        ai_trend = long_term.get("trend_pct", 0)
        ai_confidence = long_term.get("confidence", 0)
        
        print(f"\n🤖 【AI予測分析】")
        print(f"長期トレンド予測: {ai_trend:+.2f}%")
        print(f"AI信頼度: {ai_confidence:.1%}")
        
        if abs(ai_trend) < 0.1:
            print("⚠️ AI予測データに問題があります - 実際の予測値が取得できていません")
        elif ai_trend < -5:
            print(f"🚨 AI緊急警告: {ai_trend:.1f}%の大幅下落予測")
        elif ai_trend < -2:
            print(f"⚠️ AI注意: {ai_trend:.1f}%の下落予測")
        elif ai_trend > 5:
            print(f"🎉 AI楽観: {ai_trend:.1f}%の上昇予測")
        elif ai_trend > 2:
            print(f"📈 AI期待: {ai_trend:.1f}%の上昇予測")
        
        print(f"\n📊 【総合評価】")
        print(f"トレンド: {assessment.get('overall_trend', 'N/A').upper()}")
        print(f"信頼度: {assessment.get('confidence_score', 0):.1%}")
        print(f"リスク: {assessment.get('risk_level', 'N/A').upper()}")
        print(f"テクニカルスコア: {assessment.get('technical_score', 0):.2f}")
        print(f"AI信頼度: {assessment.get('ai_reliability', 0):.1%}")
        
        print(f"\n💡 【推奨アクション】")
        print(f"アクション: {advice.get('primary_action', 'N/A')}")
        print(f"強度: {advice.get('action_strength', 'N/A')}")
        
        profile_advice = advice.get("profile_adjusted_advice", {})
        if profile_advice:
            print(f"アドバイス: {profile_advice.get('advice_text', 'N/A')}")
            print(f"推奨ポジションサイズ: {profile_advice.get('position_sizing', 'N/A')}")
        
        key_factors = advice.get("key_factors", [])
        if key_factors:
            print(f"\n🔍 【主要判断要因】")
            for factor in key_factors:
                print(f"• {factor}")
        
        warnings = advice.get("warnings", [])
        if warnings:
            print(f"\n⚠️ 【警告・注意事項】")
            for i, warning in enumerate(warnings[:3], 1):
                print(f"{i}. {warning}")
        
        if market and "error" not in market:
            print(f"\n📈 【市場状況】")
            print(f"S&P500価格: ${market.get('current_price', 0):.2f}")
            print(f"日次変動: {market.get('daily_change', 0):+.2f}%")
            print(f"VIX指数: {market.get('VIX', 0):.1f} ({market.get('vix_level', 'N/A')})")
            print(f"5日ボラティリティ: {market.get('volatility_5d', 0):.1f}%")
        
        print("="*60)

    def _save_report_to_json(self, report_data: Dict[str, Any]) -> bool:
        try:
            filename = f"investment_report_{self.current_profile}_{datetime.now():%Y%m%d_%H%M%S}.json"
            return bool(self._save_analysis_report(report_data, filename))
        except Exception as e:
            self.logger.error(f"JSON保存エラー: {e}")
            return False

    def _save_analysis_report(self, report_data: Dict[str, Any], filename: str) -> Optional[str]:
        def clean_data(obj):
            if isinstance(obj, dict):
                return {k: clean_data(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_data(v) for v in obj]
            elif pd.isna(obj):
                return "N/A"
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            return obj
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(clean_data(report_data), f, ensure_ascii=False, indent=2, default=str)
            self.logger.info(f"レポート保存: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"保存エラー: {e}")
            return None

    def validate_system(self) -> Dict[str, Any]:
        validation = {
            "market_data_loaded": self.sp500_df is not None and not self.sp500_df.empty,
            "ai_models_loaded": len(self.trained_models_results) > 0,
            "profile_valid": self.current_profile in self.PROFILES,
            "config_accessible": self._get_current_config() is not None,
            "ai_predictions_working": False  # これをチェック
        }
        
        # AI予測の動作確認
        try:
            predictions, _ = self._get_ai_predictions_summary()
            long_term_trend = predictions.get("long_term", {}).get("trend_pct", 0)
            validation["ai_predictions_working"] = abs(long_term_trend) > 0.1
        except:
            pass
        
        validation["overall_valid"] = all(validation.values())
        return validation



class MarketPredictionSystem:
    """金融市場予測システムのメインクラス"""

    def __init__(
        self, config_file: str = "config.json", advisor_config_file: str = "advisor_config.json",
        logger_manager: LoggerManager = APP_LOGGER_MANAGER, # 外部からLoggerManagerを注入可能に
        reuse_hyperparams_on_init: bool = False,
        data_source_type: str = "api" # "api" or "csv"
    ):
        self.logger_manager = logger_manager
        self.logger = self.logger_manager.get_logger(self.__class__.__name__)
        self.logger.info(f"システム初期化開始。メイン設定: {config_file}, アドバイザー設定: {advisor_config_file}, データソース: {data_source_type}")

        self.config = Config(config_file, logger_manager=self.logger_manager)

        # HTTPセッション (DataFetcher API用)
        self.http_session: Optional[Any] = None # CurlSessionのインスタンス
        if CurlSession: # グローバルCurlSessionが定義されていれば
            try:
                if CurlSession.__module__.startswith("curl_cffi"):
                    self.http_session = CurlSession(impersonate="chrome110")
                    self.logger.info("HTTPセッションに curl_cffi.requests.Session を使用します。")
                else:
                    self.http_session = CurlSession()
                    self.logger.info("HTTPセッションに requests.Session を使用します。")
            except Exception as e:
                self.logger.warning(f"グローバルCurlSessionからのHTTPセッション初期化に失敗: {e}")
        else:
            self.logger.warning("CurlSessionエイリアスが未定義。APIベースのDataFetcherはHTTPセッションなしで動作します。")

        # データフェッチャーの選択
        self.data_source_type = data_source_type.lower()
        if self.data_source_type == "csv":
            self.data_fetcher: Union[CSVDataFetcher, DataFetcher] = CSVDataFetcher(self.config, self.logger_manager)
            self.logger.info("CSVDataFetcher を使用します。")
        elif self.data_source_type == "api":
            self.data_fetcher = DataFetcher(self.config, self.logger_manager, session=self.http_session)
            self.logger.info("DataFetcher (APIベース) を使用します。")
        else:
            self.logger.error(f"無効なデータソースタイプ: {data_source_type}。'api' または 'csv' を指定してください。APIをデフォルトとします。")
            self.data_fetcher = DataFetcher(self.config, self.logger_manager, session=self.http_session) # フォールバック

        self.feature_engineering = FeatureEngineering(self.config, self.logger_manager)
        self.lstm_model = LSTMModel(self.config, self.logger_manager)
        self.visualizer = MarketVisualizer(self.config, self.logger_manager)
        self.advisor_config_file = advisor_config_file # InvestmentAdvisor初期化時に渡す

        if reuse_hyperparams_on_init:
            self.lstm_model.load_best_params()

        self.market_data_store: Dict[str, Any] = {}
        self.trained_models_store: Dict[str, Any] = {}


    def run(
        self, force_hyperparam_optimization: bool = False,
        optimization_n_trials: Optional[int] = None,
        generate_report_profile: str = "natural"
    ) -> bool:
        self.logger.info(f"システム実行開始。最適化強制:{force_hyperparam_optimization}, Optuna試行:{optimization_n_trials or 'デフォルト'}, レポートプロファイル:{generate_report_profile}")
        run_start_time = datetime.now()
        overall_success = False
        try:
            # 1. データ取得
            self.logger.info("--- データ取得フェーズ開始 ---")
            self.market_data_store = self.data_fetcher.fetch_all_indexes()
            if not self.market_data_store or "^GSPC" not in self.market_data_store or self.market_data_store["^GSPC"]["df"].empty:
                self.logger.critical("主要市場データ(S&P500)取得失敗。システム続行不可。")
                return False
            self.logger.info("データ取得フェーズ完了。")

            # 2. 特徴量エンジニアリング
            self.logger.info("--- 特徴量エンジニアリングフェーズ開始 ---")
            self.market_data_store = self.feature_engineering.add_technical_indicators(self.market_data_store)
            self.logger.info("特徴量エンジニアリングフェーズ完了。")

            # 3. ハイパーパラメータ最適化 (S&P500対象)
            if force_hyperparam_optimization or (not self.lstm_model.best_params and self.lstm_model.load_best_params() is False) : # ロード試行してダメなら
                self.logger.info("--- LSTMハイパーパラメータ最適化フェーズ開始 (S&P500) ---")
                if "^GSPC" in self.market_data_store and not self.market_data_store["^GSPC"]["df"].empty:
                    self.lstm_model.optimize_hyperparameters(self.market_data_store, target_ticker="^GSPC", n_trials=optimization_n_trials)
                else: self.logger.warning("S&P500データ不十分で最適化スキップ。")
                self.logger.info("ハイパーパラメータ最適化フェーズ完了。")
            else: self.logger.info("既存ハイパーパラメータ使用または最適化要求なし。")

            # 4. LSTMモデル学習 (S&P500対象)
            self.logger.info("--- LSTMモデル学習フェーズ開始 (S&P500) ---")
            if "^GSPC" in self.market_data_store and not self.market_data_store["^GSPC"]["df"].empty:
                if not self.lstm_model.best_params: self.lstm_model.load_best_params() # 再度ロード試行
                self.trained_models_store = self.lstm_model.train_models_for_sp500(self.market_data_store)
            else: self.logger.error("S&P500データ不十分でLSTM学習スキップ。")
            self.logger.info("LSTMモデル学習フェーズ完了。")

            # 5. 可視化 (S&P500対象)
            self.logger.info("--- 可視化フェーズ開始 (S&P500) ---")
            if "^GSPC" in self.market_data_store and self.trained_models_store:
                graph_path = self.visualizer.plot_predictions_for_sp500(self.market_data_store, self.trained_models_store)
                if graph_path: self.logger.info(f"分析グラフを {graph_path} に保存。")
                else: self.logger.warning("グラフ生成/保存失敗。")
            else: self.logger.warning("S&P500データまたは学習済モデルなし。可視化スキップ。")
            self.logger.info("可視化フェーズ完了。")

            # 6. 投資アドバイス生成 (S&P500対象)
            self.logger.info("--- 投資アドバイス生成フェーズ開始 (S&P500) ---")
            if "^GSPC" in self.market_data_store and self.trained_models_store:
                advisor = InvestmentAdvisor(
                    self.market_data_store, self.trained_models_store, self.logger_manager,
                    self.advisor_config_file, generate_report_profile
                )
                if not advisor.generate_investment_report(save_to_json=True, print_to_console=True):
                    self.logger.warning("投資アドバイスレポート生成失敗。")
            else: self.logger.warning("S&P500データまたは学習済モデルなし。投資アドバイス生成スキップ。")
            self.logger.info("投資アドバイス生成フェーズ完了。")
            overall_success = True

        except KeyboardInterrupt:
            self.logger.warning("ユーザーにより処理が中断されました。")
        except Exception as e:
            self.logger.critical(f"システム実行中に致命的エラー: {e}", exc_info=True)
        finally:
            self.logger_manager.save_performance_log() # パフォーマンスログ保存
            duration_sec = (datetime.now() - run_start_time).total_seconds()
            self.logger.info(f"市場予測システム全処理終了。所要時間: {duration_sec:.2f}秒。成功: {overall_success}")
        return overall_success


# --- Jupyter Notebook / スクリプト実行のためのメイン処理部分 ---
if __name__ == "__main__":
    # グローバルなLoggerManagerインスタンスを使用
    main_logger = APP_LOGGER_MANAGER.get_logger("MainExecution")
    main_logger.info("アプリケーション実行開始。")

    # --- 設定ファイルパス (必要に応じて変更) ---
    main_config_path = "config.json"
    advisor_config_path = "advisor_config.json"
    # config.jsonのサンプル (上記デフォルト設定を参考に作成してください)
    # advisor_config.jsonのサンプル (上記デフォルト設定を参考に作成してください)

    # Jupyter Notebook環境かどうかでUIを分岐
    is_jupyter = False
    try:
        # Jupyter環境でのみ成功するインポート
        from IPython import get_ipython
        if get_ipython() is not None and 'IPKernelApp' in get_ipython().config:
            is_jupyter = True
            import ipywidgets as widgets
            from IPython.display import display, clear_output
            main_logger.info("Jupyter Notebook環境を検出。ipywidgets UIを使用します。")
    except ImportError:
        main_logger.info("Jupyter Notebook環境ではないか、ipywidgetsがありません。CUIモードで実行します。")


    if is_jupyter:
        # --- Jupyter UI ---
        data_source_selector_ui = widgets.ToggleButtons(
            options=[('API (yfinance)', 'api'), ('ローカルCSV', 'csv')], description='データソース:', value='api',
            style={'button_width': 'auto'}, tooltips=['yfinance経由で最新データを取得', '事前に用意したCSVファイルを使用']
        )
        hyperparam_mode_selector_ui = widgets.ToggleButtons(
            options=[('新規最適化', 'optimize'), ('保存パラメータ流用', 'reuse')], description='ハイパーパラメータ:', value='reuse',
            style={'button_width': 'auto'}
        )
        optuna_trials_input_ui = widgets.IntText(
            value=APP_LOGGER_MANAGER.get_logger("UI_Config").info("Optuna試行回数のデフォルト値はConfigから取得を推奨") or 5, # configから取得したい
            description='Optuna試行回数:', disabled=(hyperparam_mode_selector_ui.value == 'reuse')
        )
        def handle_hyperparam_mode_change(change): optuna_trials_input_ui.disabled = (change.new == 'reuse')
        hyperparam_mode_selector_ui.observe(handle_hyperparam_mode_change, names='value')

        # アドバイザープロファイルはAdvisorConfigLoaderから動的に取得したい
        # ここでは仮のリストを使用。システム実行時にAdvisorConfigLoaderが初期化されるので、その時点で取得するのが理想
        temp_advisor_loader = AdvisorConfigLoader(advisor_config_path, APP_LOGGER_MANAGER)
        profile_options_ui = [(p.capitalize(), p) for p in temp_advisor_loader.get_profile_list()]
        if not profile_options_ui: profile_options_ui = [('Natural', 'natural')] # フォールバック

        advisor_profile_selector_ui = widgets.Dropdown(
            options=profile_options_ui, value=profile_options_ui[0][1] if profile_options_ui else 'natural',
            description='投資判断プロファイル:', style={'description_width': 'initial'}
        )
        run_button_ui = widgets.Button(description='市場予測システム実行', button_style='success', icon='cogs')
        output_area_ui = widgets.Output()

        display(data_source_selector_ui, hyperparam_mode_selector_ui, optuna_trials_input_ui, advisor_profile_selector_ui, run_button_ui, output_area_ui)

        def on_run_button_clicked_ui(b):
            with output_area_ui:
                clear_output(wait=True)
                main_logger.info("--- UIからシステム実行開始 ---")
                data_src = data_source_selector_ui.value
                force_opt = (hyperparam_mode_selector_ui.value == 'optimize')
                opt_trials = optuna_trials_input_ui.value if force_opt else None
                report_prof = advisor_profile_selector_ui.value

                system = MarketPredictionSystem(
                    main_config_path, advisor_config_path, APP_LOGGER_MANAGER,
                    reuse_hyperparams_on_init=(not force_opt), data_source_type=data_src
                )
                success = system.run(force_opt, opt_trials, report_prof)
                print(f"\n🎉 システム実行 {'正常完了' if success else 'でエラー発生'} 🎉" if success else "\n💥 システム実行中にエラーが発生しました 💥")
        run_button_ui.on_click(on_run_button_clicked_ui)

    else:
        # --- CUIフォールバック ---
        print("="*30 + "\n金融市場予測システム (CUIモード)\n" + "="*30)
        ds_input = input("データソースを選択 [api, csv] (デフォルト: api): ").strip().lower() or "api"
        mode_input = input("ハイパーパラメータモードを選択 [optimize, reuse] (デフォルト: reuse): ").strip().lower() or "reuse"
        force_opt_cui = (mode_input == "optimize")
        opt_trials_cui_val = None
        if force_opt_cui:
            try: opt_trials_cui_val = int(input("Optuna試行回数を入力 (デフォルト: 5): ").strip() or "5")
            except ValueError: opt_trials_cui_val = 5

        # アドバイザープロファイル (CUI)
        temp_advisor_loader_cui = AdvisorConfigLoader(advisor_config_path, APP_LOGGER_MANAGER)
        profiles_cui = temp_advisor_loader_cui.get_profile_list()
        profile_prompt = f"投資判断プロファイルを選択 [{', '.join(profiles_cui)}] (デフォルト: {profiles_cui[0] if profiles_cui else 'natural'}): "
        profile_input_cui = input(profile_prompt).strip().lower() or (profiles_cui[0] if profiles_cui else 'natural')
        if profile_input_cui not in profiles_cui and profiles_cui : profile_input_cui = profiles_cui[0] # 不正入力時は先頭

        system_cui = MarketPredictionSystem(
            main_config_path, advisor_config_path, APP_LOGGER_MANAGER,
            reuse_hyperparams_on_init=(not force_opt_cui), data_source_type=ds_input
        )
        system_cui.run(force_opt_cui, opt_trials_cui_val, profile_input_cui)

    main_logger.info("アプリケーション実行終了。")
